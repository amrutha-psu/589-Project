{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQzvjy5uZH-C",
        "outputId": "eb36fd39-e769-4ba7-899a-258bde4c0c59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'PromptSRC'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Counting objects: 100% (236/236), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 236 (delta 88), reused 199 (delta 67), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (236/236), 32.99 MiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "Cloning into 'Dassl.pytorch'...\n",
            "remote: Enumerating objects: 2477, done.\u001b[K\n",
            "remote: Counting objects: 100% (1081/1081), done.\u001b[K\n",
            "remote: Compressing objects: 100% (235/235), done.\u001b[K\n",
            "remote: Total 2477 (delta 933), reused 846 (delta 846), pack-reused 1396 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2477/2477), 410.19 KiB | 18.64 MiB/s, done.\n",
            "Resolving deltas: 100% (1676/1676), done.\n",
            "/content/Dassl.pytorch\n",
            "/content/PromptSRC\n",
            "üîß Patching library for ViT-L/14 support...\n",
            "‚úÖ Library patched successfully.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   348  100   348    0     0    277      0  0:00:01  0:00:01 --:--:--   277\n",
            "100  328M  100  328M    0     0  17.2M      0  0:00:19  0:00:19 --:--:-- 21.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   349  100   349    0     0    468      0 --:--:-- --:--:-- --:--:--   467\n",
            "100   502  100   502    0     0    336      0  0:00:01  0:00:01 --:--:--     0\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   343  100   343    0     0    462      0 --:--:-- --:--:-- --:--:--   462\n",
            "100 14989  100 14989    0     0   9071      0  0:00:01  0:00:01 --:--:-- 20310\n",
            "üöÄ RUNNING SUBSTANTIAL IMPROVEMENT EXPERIMENT (ViT-L/14)\n",
            "‚è≥ This involves training from scratch. Please wait ~30-40 mins.\n",
            "üöÄ STARTING TRAINING (ViT-L/14)...\n",
            "2025-12-09 04:43:48.792383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765255428.812004    1490 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765255428.818145    1490 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765255428.832763    1490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765255428.832787    1490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765255428.832791    1490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765255428.832794    1490 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 04:43:48.837188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/PromptSRC/vit_l14_flowers.yaml\n",
            "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
            "eval_only: False\n",
            "head: \n",
            "load_epoch: None\n",
            "model_dir: \n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base', 'DATALOADER.NUM_WORKERS', '0']\n",
            "output_dir: output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1\n",
            "resume: \n",
            "root: /content/PromptSRC/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: PromptSRC\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 0\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 50\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 2\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordFlowers\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/PromptSRC/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: base\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-L/14\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.0025\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: PromptSRC\n",
            "  PROMPTSRC:\n",
            "    CTX_INIT: a photo of a\n",
            "    GPA_MEAN: 45.0\n",
            "    GPA_STD: 5.0\n",
            "    IMAGE_LOSS_WEIGHT: 10.0\n",
            "    N_CTX: 4\n",
            "    N_CTX_TEXT: 4\n",
            "    N_CTX_VISION: 4\n",
            "    PREC: fp16\n",
            "    PROMPT_DEPTH_TEXT: 9\n",
            "    PROMPT_DEPTH_VISION: 9\n",
            "    TEXT_LOSS_WEIGHT: 25.0\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.9.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.10\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: \n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "Is XPU available: False\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.24\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] intel-cmplr-lib-ur==2025.3.1\n",
            "[pip3] intel-openmp==2025.3.1\n",
            "[pip3] mkl==2025.3.0\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.14\n",
            "[pip3] onemkl-license==2025.3.0\n",
            "[pip3] optree==0.18.0\n",
            "[pip3] tbb==2022.3.0\n",
            "[pip3] tcmlib==1.4.1\n",
            "[pip3] torch==2.9.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.9.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.24.0+cu126\n",
            "[pip3] triton==3.5.0\n",
            "[pip3] umf==1.0.2\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "Loading trainer: PromptSRC\n",
            "Loading dataset: OxfordFlowers\n",
            "Splitting data into 50% train, 20% val, and 30% test\n",
            "Saved split to /content/PromptSRC/data/oxford_flowers/split_zhou_OxfordFlowers.json\n",
            "Creating a 16-shot dataset\n",
            "Creating a 4-shot dataset\n",
            "Saving preprocessed few-shot data to /content/PromptSRC/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE BASE CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------------\n",
            "Dataset    OxfordFlowers\n",
            "# classes  51\n",
            "# train_x  816\n",
            "# val      204\n",
            "# test     1,053\n",
            "---------  -------------\n",
            "Loading CLIP (backbone: ViT-L/14)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 933M/933M [00:06<00:00, 139MiB/s]\n",
            "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
            "Building custom CLIP\n",
            "Independent V-L design\n",
            "Initial text context: \"a photo of a\"\n",
            "Number of context words (tokens) for Language prompting: 4\n",
            "Number of context words (tokens) for Vision prompting: 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'image_encoder.transformer.resblocks.7.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.VPT', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'prompt_learner.ctx', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow'}\n",
            "Parameters count: 18\n",
            "Loading evaluator: Classification\n",
            "No checkpoint found, train from scratch\n",
            "Initialize tensorboard (log_dir=output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1/tensorboard)\n",
            "epoch [1/50] batch [20/408] time 0.218 (0.278) data 0.009 (0.015) loss 0.9113 (2.3675) lr 1.0000e-05 eta 1:34:34\n",
            "epoch [1/50] batch [40/408] time 0.228 (0.251) data 0.015 (0.014) loss 2.5959 (2.0821) lr 1.0000e-05 eta 1:25:03\n",
            "epoch [1/50] batch [60/408] time 0.222 (0.241) data 0.013 (0.014) loss 0.6279 (1.9233) lr 1.0000e-05 eta 1:21:49\n",
            "epoch [1/50] batch [80/408] time 0.220 (0.237) data 0.009 (0.013) loss 4.1481 (2.0217) lr 1.0000e-05 eta 1:20:12\n",
            "epoch [1/50] batch [100/408] time 0.225 (0.235) data 0.011 (0.013) loss 0.4310 (1.9209) lr 1.0000e-05 eta 1:19:33\n",
            "epoch [1/50] batch [120/408] time 0.224 (0.233) data 0.012 (0.013) loss 0.8329 (1.8089) lr 1.0000e-05 eta 1:18:54\n",
            "epoch [1/50] batch [140/408] time 0.232 (0.232) data 0.011 (0.013) loss 1.5004 (1.7311) lr 1.0000e-05 eta 1:18:28\n",
            "epoch [1/50] batch [160/408] time 0.226 (0.232) data 0.010 (0.013) loss 1.9182 (1.7072) lr 1.0000e-05 eta 1:18:21\n",
            "epoch [1/50] batch [180/408] time 0.228 (0.232) data 0.010 (0.013) loss 2.3797 (1.7128) lr 1.0000e-05 eta 1:18:09\n",
            "epoch [1/50] batch [200/408] time 0.239 (0.232) data 0.019 (0.013) loss 0.5791 (1.6764) lr 1.0000e-05 eta 1:18:07\n",
            "epoch [1/50] batch [220/408] time 0.234 (0.232) data 0.013 (0.013) loss 3.0383 (1.6200) lr 1.0000e-05 eta 1:18:01\n",
            "epoch [1/50] batch [240/408] time 0.231 (0.232) data 0.010 (0.013) loss 2.7884 (1.5729) lr 1.0000e-05 eta 1:17:56\n",
            "epoch [1/50] batch [260/408] time 0.234 (0.232) data 0.011 (0.013) loss 0.7111 (1.5311) lr 1.0000e-05 eta 1:17:57\n",
            "epoch [1/50] batch [280/408] time 0.235 (0.232) data 0.011 (0.013) loss 1.3787 (1.5365) lr 1.0000e-05 eta 1:17:55\n",
            "epoch [1/50] batch [300/408] time 0.243 (0.233) data 0.019 (0.013) loss 1.9869 (1.5360) lr 1.0000e-05 eta 1:17:57\n",
            "epoch [1/50] batch [320/408] time 0.243 (0.233) data 0.012 (0.013) loss 0.4004 (1.5103) lr 1.0000e-05 eta 1:18:00\n",
            "epoch [1/50] batch [340/408] time 0.236 (0.233) data 0.011 (0.013) loss 0.6168 (1.4704) lr 1.0000e-05 eta 1:18:00\n",
            "epoch [1/50] batch [360/408] time 0.236 (0.234) data 0.011 (0.013) loss 0.5834 (1.4697) lr 1.0000e-05 eta 1:18:04\n",
            "epoch [1/50] batch [380/408] time 0.240 (0.234) data 0.013 (0.013) loss 0.4524 (1.4471) lr 1.0000e-05 eta 1:18:04\n",
            "epoch [1/50] batch [400/408] time 0.243 (0.234) data 0.018 (0.013) loss 1.8780 (1.4385) lr 1.0000e-05 eta 1:18:05\n",
            "epoch [2/50] batch [20/408] time 0.236 (0.235) data 0.013 (0.012) loss 3.5328 (1.4247) lr 2.5000e-03 eta 1:18:18\n",
            "epoch [2/50] batch [40/408] time 0.239 (0.235) data 0.016 (0.012) loss 0.9799 (1.4136) lr 2.5000e-03 eta 1:18:18\n",
            "epoch [2/50] batch [60/408] time 0.234 (0.236) data 0.011 (0.013) loss 3.8163 (1.4530) lr 2.5000e-03 eta 1:18:22\n",
            "epoch [2/50] batch [80/408] time 0.232 (0.235) data 0.008 (0.013) loss 4.8436 (1.3847) lr 2.5000e-03 eta 1:18:01\n",
            "epoch [2/50] batch [100/408] time 0.237 (0.235) data 0.011 (0.013) loss 3.9343 (1.3458) lr 2.5000e-03 eta 1:18:03\n",
            "epoch [2/50] batch [120/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.8773 (1.2808) lr 2.5000e-03 eta 1:17:48\n",
            "epoch [2/50] batch [140/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.8021 (1.3054) lr 2.5000e-03 eta 1:17:37\n",
            "epoch [2/50] batch [160/408] time 0.230 (0.235) data 0.010 (0.013) loss 1.6774 (1.2910) lr 2.5000e-03 eta 1:17:36\n",
            "epoch [2/50] batch [180/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.5025 (1.2659) lr 2.5000e-03 eta 1:17:26\n",
            "epoch [2/50] batch [200/408] time 0.243 (0.235) data 0.017 (0.013) loss 0.4859 (1.2490) lr 2.5000e-03 eta 1:17:26\n",
            "epoch [2/50] batch [220/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.4744 (1.2760) lr 2.5000e-03 eta 1:17:22\n",
            "epoch [2/50] batch [240/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.4353 (1.2234) lr 2.5000e-03 eta 1:17:17\n",
            "epoch [2/50] batch [260/408] time 0.233 (0.235) data 0.011 (0.013) loss 1.0560 (1.2362) lr 2.5000e-03 eta 1:17:18\n",
            "epoch [2/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.5152 (1.2054) lr 2.5000e-03 eta 1:17:14\n",
            "epoch [2/50] batch [300/408] time 0.242 (0.235) data 0.019 (0.013) loss 0.4302 (1.1948) lr 2.5000e-03 eta 1:17:13\n",
            "epoch [2/50] batch [320/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.7815 (1.1700) lr 2.5000e-03 eta 1:17:11\n",
            "epoch [2/50] batch [340/408] time 0.237 (0.235) data 0.011 (0.013) loss 0.3785 (1.1477) lr 2.5000e-03 eta 1:17:07\n",
            "epoch [2/50] batch [360/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.6788 (1.1293) lr 2.5000e-03 eta 1:17:07\n",
            "epoch [2/50] batch [380/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.6131 (1.1226) lr 2.5000e-03 eta 1:17:02\n",
            "epoch [2/50] batch [400/408] time 0.239 (0.236) data 0.017 (0.013) loss 1.0718 (1.1101) lr 2.5000e-03 eta 1:16:57\n",
            "epoch [3/50] batch [20/408] time 0.234 (0.235) data 0.013 (0.012) loss 0.5817 (0.6731) lr 2.4975e-03 eta 1:16:29\n",
            "epoch [3/50] batch [40/408] time 0.244 (0.235) data 0.023 (0.012) loss 1.6182 (0.7573) lr 2.4975e-03 eta 1:16:27\n",
            "epoch [3/50] batch [60/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.3316 (0.8242) lr 2.4975e-03 eta 1:16:35\n",
            "epoch [3/50] batch [80/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.3219 (0.8118) lr 2.4975e-03 eta 1:16:24\n",
            "epoch [3/50] batch [100/408] time 0.235 (0.235) data 0.011 (0.013) loss 1.7023 (0.7925) lr 2.4975e-03 eta 1:16:26\n",
            "epoch [3/50] batch [120/408] time 0.231 (0.235) data 0.010 (0.013) loss 2.5684 (0.8246) lr 2.4975e-03 eta 1:16:14\n",
            "epoch [3/50] batch [140/408] time 0.237 (0.235) data 0.015 (0.013) loss 0.4658 (0.8208) lr 2.4975e-03 eta 1:16:06\n",
            "epoch [3/50] batch [160/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.5203 (0.8078) lr 2.4975e-03 eta 1:16:09\n",
            "epoch [3/50] batch [180/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.3087 (0.7862) lr 2.4975e-03 eta 1:16:05\n",
            "epoch [3/50] batch [200/408] time 0.244 (0.236) data 0.020 (0.013) loss 0.6389 (0.7724) lr 2.4975e-03 eta 1:16:07\n",
            "epoch [3/50] batch [220/408] time 0.233 (0.236) data 0.013 (0.013) loss 0.3314 (0.7587) lr 2.4975e-03 eta 1:16:02\n",
            "epoch [3/50] batch [240/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.3487 (0.7329) lr 2.4975e-03 eta 1:15:56\n",
            "epoch [3/50] batch [260/408] time 0.231 (0.236) data 0.010 (0.013) loss 0.2861 (0.7181) lr 2.4975e-03 eta 1:15:55\n",
            "epoch [3/50] batch [280/408] time 0.242 (0.236) data 0.018 (0.013) loss 0.2855 (0.7089) lr 2.4975e-03 eta 1:15:49\n",
            "epoch [3/50] batch [300/408] time 0.236 (0.236) data 0.014 (0.013) loss 1.6274 (0.7144) lr 2.4975e-03 eta 1:15:44\n",
            "epoch [3/50] batch [320/408] time 0.235 (0.236) data 0.011 (0.013) loss 0.3532 (0.7148) lr 2.4975e-03 eta 1:15:41\n",
            "epoch [3/50] batch [340/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.3994 (0.7137) lr 2.4975e-03 eta 1:15:35\n",
            "epoch [3/50] batch [360/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.3314 (0.7061) lr 2.4975e-03 eta 1:15:32\n",
            "epoch [3/50] batch [380/408] time 0.233 (0.236) data 0.012 (0.013) loss 0.4197 (0.7025) lr 2.4975e-03 eta 1:15:26\n",
            "epoch [3/50] batch [400/408] time 0.242 (0.236) data 0.017 (0.013) loss 0.6218 (0.6876) lr 2.4975e-03 eta 1:15:23\n",
            "epoch [4/50] batch [20/408] time 0.233 (0.236) data 0.011 (0.012) loss 1.0557 (0.5266) lr 2.4901e-03 eta 1:15:16\n",
            "epoch [4/50] batch [40/408] time 0.240 (0.236) data 0.017 (0.013) loss 1.2620 (0.5443) lr 2.4901e-03 eta 1:15:21\n",
            "epoch [4/50] batch [60/408] time 0.245 (0.236) data 0.021 (0.013) loss 0.4169 (0.5339) lr 2.4901e-03 eta 1:15:19\n",
            "epoch [4/50] batch [80/408] time 0.235 (0.236) data 0.013 (0.013) loss 1.1504 (0.6013) lr 2.4901e-03 eta 1:15:06\n",
            "epoch [4/50] batch [100/408] time 0.238 (0.236) data 0.013 (0.013) loss 0.4066 (0.6003) lr 2.4901e-03 eta 1:15:03\n",
            "epoch [4/50] batch [120/408] time 0.235 (0.236) data 0.011 (0.013) loss 0.3883 (0.5645) lr 2.4901e-03 eta 1:14:55\n",
            "epoch [4/50] batch [140/408] time 0.241 (0.236) data 0.018 (0.013) loss 0.3977 (0.5520) lr 2.4901e-03 eta 1:14:52\n",
            "epoch [4/50] batch [160/408] time 0.235 (0.236) data 0.011 (0.013) loss 0.4873 (0.5518) lr 2.4901e-03 eta 1:14:47\n",
            "epoch [4/50] batch [180/408] time 0.236 (0.236) data 0.012 (0.013) loss 0.2944 (0.5456) lr 2.4901e-03 eta 1:14:41\n",
            "epoch [4/50] batch [200/408] time 0.236 (0.236) data 0.012 (0.013) loss 0.2777 (0.5305) lr 2.4901e-03 eta 1:14:40\n",
            "epoch [4/50] batch [220/408] time 0.233 (0.236) data 0.012 (0.013) loss 0.3462 (0.5320) lr 2.4901e-03 eta 1:14:33\n",
            "epoch [4/50] batch [240/408] time 0.244 (0.236) data 0.023 (0.013) loss 0.3057 (0.5447) lr 2.4901e-03 eta 1:14:27\n",
            "epoch [4/50] batch [260/408] time 0.235 (0.236) data 0.012 (0.013) loss 1.3737 (0.5578) lr 2.4901e-03 eta 1:14:24\n",
            "epoch [4/50] batch [280/408] time 0.232 (0.236) data 0.012 (0.013) loss 0.8309 (0.5685) lr 2.4901e-03 eta 1:14:19\n",
            "epoch [4/50] batch [300/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.3306 (0.5689) lr 2.4901e-03 eta 1:14:17\n",
            "epoch [4/50] batch [320/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.6921 (0.5682) lr 2.4901e-03 eta 1:14:13\n",
            "epoch [4/50] batch [340/408] time 0.239 (0.236) data 0.014 (0.013) loss 3.5698 (0.5847) lr 2.4901e-03 eta 1:14:07\n",
            "epoch [4/50] batch [360/408] time 0.236 (0.236) data 0.013 (0.013) loss 0.3627 (0.5776) lr 2.4901e-03 eta 1:14:02\n",
            "epoch [4/50] batch [380/408] time 0.235 (0.236) data 0.011 (0.013) loss 0.2863 (0.5762) lr 2.4901e-03 eta 1:13:56\n",
            "epoch [4/50] batch [400/408] time 0.236 (0.236) data 0.010 (0.013) loss 0.9868 (0.5746) lr 2.4901e-03 eta 1:13:53\n",
            "epoch [5/50] batch [20/408] time 0.233 (0.234) data 0.010 (0.012) loss 0.5090 (0.6376) lr 2.4779e-03 eta 1:13:09\n",
            "epoch [5/50] batch [40/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.3369 (0.6099) lr 2.4779e-03 eta 1:13:26\n",
            "epoch [5/50] batch [60/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3467 (0.6258) lr 2.4779e-03 eta 1:13:16\n",
            "epoch [5/50] batch [80/408] time 0.245 (0.235) data 0.019 (0.013) loss 0.2830 (0.5554) lr 2.4779e-03 eta 1:13:04\n",
            "epoch [5/50] batch [100/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.3164 (0.5256) lr 2.4779e-03 eta 1:13:09\n",
            "epoch [5/50] batch [120/408] time 0.234 (0.235) data 0.011 (0.013) loss 1.1427 (0.5585) lr 2.4779e-03 eta 1:13:02\n",
            "epoch [5/50] batch [140/408] time 0.233 (0.235) data 0.011 (0.013) loss 1.2433 (0.5582) lr 2.4779e-03 eta 1:12:59\n",
            "epoch [5/50] batch [160/408] time 0.236 (0.235) data 0.010 (0.013) loss 0.6472 (0.5330) lr 2.4779e-03 eta 1:12:54\n",
            "epoch [5/50] batch [180/408] time 0.239 (0.235) data 0.012 (0.013) loss 0.4746 (0.5092) lr 2.4779e-03 eta 1:12:47\n",
            "epoch [5/50] batch [200/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.6814 (0.5060) lr 2.4779e-03 eta 1:12:46\n",
            "epoch [5/50] batch [220/408] time 0.233 (0.235) data 0.009 (0.013) loss 0.8020 (0.5145) lr 2.4779e-03 eta 1:12:38\n",
            "epoch [5/50] batch [240/408] time 0.232 (0.235) data 0.013 (0.013) loss 0.3758 (0.5004) lr 2.4779e-03 eta 1:12:36\n",
            "epoch [5/50] batch [260/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.3026 (0.5083) lr 2.4779e-03 eta 1:12:29\n",
            "epoch [5/50] batch [280/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.9866 (0.5104) lr 2.4779e-03 eta 1:12:23\n",
            "epoch [5/50] batch [300/408] time 0.230 (0.235) data 0.010 (0.013) loss 0.3173 (0.5059) lr 2.4779e-03 eta 1:12:21\n",
            "epoch [5/50] batch [320/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.4362 (0.4989) lr 2.4779e-03 eta 1:12:15\n",
            "epoch [5/50] batch [340/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.5904 (0.4950) lr 2.4779e-03 eta 1:12:14\n",
            "epoch [5/50] batch [360/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2559 (0.4972) lr 2.4779e-03 eta 1:12:07\n",
            "epoch [5/50] batch [380/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.7521 (0.5113) lr 2.4779e-03 eta 1:12:02\n",
            "epoch [5/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2650 (0.5059) lr 2.4779e-03 eta 1:11:59\n",
            "epoch [6/50] batch [20/408] time 0.234 (0.235) data 0.013 (0.012) loss 0.2815 (0.4712) lr 2.4607e-03 eta 1:11:54\n",
            "epoch [6/50] batch [40/408] time 0.234 (0.237) data 0.011 (0.014) loss 1.1451 (0.4817) lr 2.4607e-03 eta 1:12:16\n",
            "epoch [6/50] batch [60/408] time 0.234 (0.236) data 0.014 (0.013) loss 1.1701 (0.4995) lr 2.4607e-03 eta 1:11:59\n",
            "epoch [6/50] batch [80/408] time 0.239 (0.236) data 0.018 (0.013) loss 0.2915 (0.4669) lr 2.4607e-03 eta 1:11:52\n",
            "epoch [6/50] batch [100/408] time 0.236 (0.236) data 0.011 (0.013) loss 0.4777 (0.4763) lr 2.4607e-03 eta 1:11:43\n",
            "epoch [6/50] batch [120/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2838 (0.4596) lr 2.4607e-03 eta 1:11:35\n",
            "epoch [6/50] batch [140/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.2509 (0.4656) lr 2.4607e-03 eta 1:11:37\n",
            "epoch [6/50] batch [160/408] time 0.236 (0.236) data 0.013 (0.013) loss 0.2405 (0.4582) lr 2.4607e-03 eta 1:11:32\n",
            "epoch [6/50] batch [180/408] time 0.236 (0.236) data 0.016 (0.013) loss 0.2480 (0.4661) lr 2.4607e-03 eta 1:11:29\n",
            "epoch [6/50] batch [200/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.3096 (0.4529) lr 2.4607e-03 eta 1:11:21\n",
            "epoch [6/50] batch [220/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.5055 (0.4546) lr 2.4607e-03 eta 1:11:15\n",
            "epoch [6/50] batch [240/408] time 0.236 (0.236) data 0.013 (0.013) loss 2.1114 (0.4509) lr 2.4607e-03 eta 1:11:13\n",
            "epoch [6/50] batch [260/408] time 0.233 (0.236) data 0.012 (0.013) loss 0.2665 (0.4416) lr 2.4607e-03 eta 1:11:06\n",
            "epoch [6/50] batch [280/408] time 0.241 (0.236) data 0.017 (0.013) loss 0.2728 (0.4417) lr 2.4607e-03 eta 1:11:02\n",
            "epoch [6/50] batch [300/408] time 0.237 (0.236) data 0.014 (0.013) loss 0.4239 (0.4467) lr 2.4607e-03 eta 1:10:55\n",
            "epoch [6/50] batch [320/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.3082 (0.4468) lr 2.4607e-03 eta 1:10:50\n",
            "epoch [6/50] batch [340/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.2603 (0.4459) lr 2.4607e-03 eta 1:10:47\n",
            "epoch [6/50] batch [360/408] time 0.236 (0.236) data 0.010 (0.013) loss 0.7591 (0.4472) lr 2.4607e-03 eta 1:10:41\n",
            "epoch [6/50] batch [380/408] time 0.243 (0.236) data 0.018 (0.013) loss 2.2412 (0.4621) lr 2.4607e-03 eta 1:10:37\n",
            "epoch [6/50] batch [400/408] time 0.239 (0.236) data 0.018 (0.013) loss 0.2788 (0.4662) lr 2.4607e-03 eta 1:10:32\n",
            "epoch [7/50] batch [20/408] time 0.241 (0.236) data 0.019 (0.013) loss 0.4016 (0.4252) lr 2.4388e-03 eta 1:10:31\n",
            "epoch [7/50] batch [40/408] time 0.242 (0.236) data 0.015 (0.013) loss 0.4224 (0.5133) lr 2.4388e-03 eta 1:10:18\n",
            "epoch [7/50] batch [60/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2656 (0.5080) lr 2.4388e-03 eta 1:10:06\n",
            "epoch [7/50] batch [80/408] time 0.233 (0.235) data 0.011 (0.013) loss 1.0326 (0.5120) lr 2.4388e-03 eta 1:10:04\n",
            "epoch [7/50] batch [100/408] time 0.232 (0.235) data 0.011 (0.013) loss 2.1711 (0.5025) lr 2.4388e-03 eta 1:09:57\n",
            "epoch [7/50] batch [120/408] time 0.243 (0.235) data 0.014 (0.012) loss 0.3294 (0.5063) lr 2.4388e-03 eta 1:09:51\n",
            "epoch [7/50] batch [140/408] time 0.231 (0.235) data 0.011 (0.012) loss 0.3719 (0.4961) lr 2.4388e-03 eta 1:09:46\n",
            "epoch [7/50] batch [160/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.4561 (0.4861) lr 2.4388e-03 eta 1:09:39\n",
            "epoch [7/50] batch [180/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2995 (0.4847) lr 2.4388e-03 eta 1:09:38\n",
            "epoch [7/50] batch [200/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3588 (0.4751) lr 2.4388e-03 eta 1:09:32\n",
            "epoch [7/50] batch [220/408] time 0.243 (0.235) data 0.020 (0.013) loss 1.1040 (0.4814) lr 2.4388e-03 eta 1:09:28\n",
            "epoch [7/50] batch [240/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2307 (0.4772) lr 2.4388e-03 eta 1:09:25\n",
            "epoch [7/50] batch [260/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2312 (0.4676) lr 2.4388e-03 eta 1:09:18\n",
            "epoch [7/50] batch [280/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.3481 (0.4628) lr 2.4388e-03 eta 1:09:15\n",
            "epoch [7/50] batch [300/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3501 (0.4534) lr 2.4388e-03 eta 1:09:09\n",
            "epoch [7/50] batch [320/408] time 0.240 (0.235) data 0.015 (0.013) loss 0.2494 (0.4534) lr 2.4388e-03 eta 1:09:04\n",
            "epoch [7/50] batch [340/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2290 (0.4490) lr 2.4388e-03 eta 1:09:00\n",
            "epoch [7/50] batch [360/408] time 0.232 (0.235) data 0.010 (0.013) loss 1.7579 (0.4537) lr 2.4388e-03 eta 1:08:55\n",
            "epoch [7/50] batch [380/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.4019 (0.4474) lr 2.4388e-03 eta 1:08:51\n",
            "epoch [7/50] batch [400/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.7825 (0.4425) lr 2.4388e-03 eta 1:08:46\n",
            "epoch [8/50] batch [20/408] time 0.232 (0.238) data 0.013 (0.015) loss 0.3357 (0.4951) lr 2.4122e-03 eta 1:09:22\n",
            "epoch [8/50] batch [40/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.2791 (0.5029) lr 2.4122e-03 eta 1:08:44\n",
            "epoch [8/50] batch [60/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.3252 (0.5398) lr 2.4122e-03 eta 1:08:28\n",
            "epoch [8/50] batch [80/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.4056 (0.4964) lr 2.4122e-03 eta 1:08:29\n",
            "epoch [8/50] batch [100/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2452 (0.5055) lr 2.4122e-03 eta 1:08:17\n",
            "epoch [8/50] batch [120/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.5544 (0.4949) lr 2.4122e-03 eta 1:08:16\n",
            "epoch [8/50] batch [140/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2411 (0.4695) lr 2.4122e-03 eta 1:08:11\n",
            "epoch [8/50] batch [160/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2795 (0.4710) lr 2.4122e-03 eta 1:08:03\n",
            "epoch [8/50] batch [180/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3078 (0.4700) lr 2.4122e-03 eta 1:08:03\n",
            "epoch [8/50] batch [200/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.3277 (0.4713) lr 2.4122e-03 eta 1:07:56\n",
            "epoch [8/50] batch [220/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2352 (0.4887) lr 2.4122e-03 eta 1:07:53\n",
            "epoch [8/50] batch [240/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3191 (0.4854) lr 2.4122e-03 eta 1:07:48\n",
            "epoch [8/50] batch [260/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2447 (0.4851) lr 2.4122e-03 eta 1:07:42\n",
            "epoch [8/50] batch [280/408] time 0.230 (0.235) data 0.010 (0.013) loss 0.3319 (0.4842) lr 2.4122e-03 eta 1:07:39\n",
            "epoch [8/50] batch [300/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2827 (0.4806) lr 2.4122e-03 eta 1:07:33\n",
            "epoch [8/50] batch [320/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.5218 (0.5017) lr 2.4122e-03 eta 1:07:30\n",
            "epoch [8/50] batch [340/408] time 0.235 (0.235) data 0.013 (0.013) loss 1.4766 (0.5034) lr 2.4122e-03 eta 1:07:24\n",
            "epoch [8/50] batch [360/408] time 0.234 (0.235) data 0.010 (0.013) loss 1.6879 (0.5017) lr 2.4122e-03 eta 1:07:19\n",
            "epoch [8/50] batch [380/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2738 (0.4950) lr 2.4122e-03 eta 1:07:16\n",
            "epoch [8/50] batch [400/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.2481 (0.4875) lr 2.4122e-03 eta 1:07:11\n",
            "epoch [9/50] batch [20/408] time 0.235 (0.238) data 0.014 (0.015) loss 0.2324 (0.3190) lr 2.3810e-03 eta 1:07:48\n",
            "epoch [9/50] batch [40/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.6386 (0.4157) lr 2.3810e-03 eta 1:07:12\n",
            "epoch [9/50] batch [60/408] time 0.238 (0.236) data 0.018 (0.014) loss 0.3237 (0.4262) lr 2.3810e-03 eta 1:07:16\n",
            "epoch [9/50] batch [80/408] time 0.232 (0.236) data 0.011 (0.013) loss 0.2865 (0.4126) lr 2.3810e-03 eta 1:07:05\n",
            "epoch [9/50] batch [100/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.2355 (0.4128) lr 2.3810e-03 eta 1:06:54\n",
            "epoch [9/50] batch [120/408] time 0.236 (0.236) data 0.013 (0.013) loss 0.3201 (0.4078) lr 2.3810e-03 eta 1:06:52\n",
            "epoch [9/50] batch [140/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.2663 (0.3955) lr 2.3810e-03 eta 1:06:44\n",
            "epoch [9/50] batch [160/408] time 0.239 (0.236) data 0.016 (0.013) loss 0.3327 (0.3968) lr 2.3810e-03 eta 1:06:39\n",
            "epoch [9/50] batch [180/408] time 0.232 (0.236) data 0.010 (0.013) loss 0.4690 (0.4025) lr 2.3810e-03 eta 1:06:33\n",
            "epoch [9/50] batch [200/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.3100 (0.4213) lr 2.3810e-03 eta 1:06:27\n",
            "epoch [9/50] batch [220/408] time 0.237 (0.236) data 0.013 (0.013) loss 0.2770 (0.4297) lr 2.3810e-03 eta 1:06:25\n",
            "epoch [9/50] batch [240/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.3510 (0.4291) lr 2.3810e-03 eta 1:06:18\n",
            "epoch [9/50] batch [260/408] time 0.239 (0.235) data 0.018 (0.013) loss 0.3001 (0.4290) lr 2.3810e-03 eta 1:06:12\n",
            "epoch [9/50] batch [280/408] time 0.237 (0.235) data 0.016 (0.013) loss 0.3247 (0.4208) lr 2.3810e-03 eta 1:06:07\n",
            "epoch [9/50] batch [300/408] time 0.238 (0.235) data 0.014 (0.013) loss 0.2844 (0.4260) lr 2.3810e-03 eta 1:06:01\n",
            "epoch [9/50] batch [320/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2116 (0.4212) lr 2.3810e-03 eta 1:05:58\n",
            "epoch [9/50] batch [340/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2716 (0.4242) lr 2.3810e-03 eta 1:05:52\n",
            "epoch [9/50] batch [360/408] time 0.237 (0.235) data 0.016 (0.013) loss 0.6675 (0.4335) lr 2.3810e-03 eta 1:05:46\n",
            "epoch [9/50] batch [380/408] time 0.230 (0.235) data 0.010 (0.013) loss 0.5731 (0.4346) lr 2.3810e-03 eta 1:05:41\n",
            "epoch [9/50] batch [400/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.4165 (0.4296) lr 2.3810e-03 eta 1:05:35\n",
            "epoch [10/50] batch [20/408] time 0.239 (0.235) data 0.011 (0.013) loss 0.2863 (0.3994) lr 2.3454e-03 eta 1:05:23\n",
            "epoch [10/50] batch [40/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2619 (0.3564) lr 2.3454e-03 eta 1:05:07\n",
            "epoch [10/50] batch [60/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.3278 (0.3920) lr 2.3454e-03 eta 1:05:10\n",
            "epoch [10/50] batch [80/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2705 (0.3989) lr 2.3454e-03 eta 1:05:04\n",
            "epoch [10/50] batch [100/408] time 0.237 (0.235) data 0.015 (0.013) loss 0.3677 (0.3868) lr 2.3454e-03 eta 1:05:02\n",
            "epoch [10/50] batch [120/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.3094 (0.4403) lr 2.3454e-03 eta 1:05:00\n",
            "epoch [10/50] batch [140/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.3118 (0.4249) lr 2.3454e-03 eta 1:04:54\n",
            "epoch [10/50] batch [160/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2782 (0.4182) lr 2.3454e-03 eta 1:04:53\n",
            "epoch [10/50] batch [180/408] time 0.237 (0.235) data 0.015 (0.013) loss 0.2700 (0.4281) lr 2.3454e-03 eta 1:04:46\n",
            "epoch [10/50] batch [200/408] time 0.237 (0.235) data 0.016 (0.013) loss 0.4295 (0.4344) lr 2.3454e-03 eta 1:04:42\n",
            "epoch [10/50] batch [220/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.6467 (0.4309) lr 2.3454e-03 eta 1:04:37\n",
            "epoch [10/50] batch [240/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2994 (0.4324) lr 2.3454e-03 eta 1:04:32\n",
            "epoch [10/50] batch [260/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.3135 (0.4267) lr 2.3454e-03 eta 1:04:29\n",
            "epoch [10/50] batch [280/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.4047 (0.4186) lr 2.3454e-03 eta 1:04:23\n",
            "epoch [10/50] batch [300/408] time 0.241 (0.235) data 0.015 (0.013) loss 0.9536 (0.4143) lr 2.3454e-03 eta 1:04:18\n",
            "epoch [10/50] batch [320/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.5128 (0.4110) lr 2.3454e-03 eta 1:04:14\n",
            "epoch [10/50] batch [340/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2697 (0.4260) lr 2.3454e-03 eta 1:04:08\n",
            "epoch [10/50] batch [360/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2215 (0.4310) lr 2.3454e-03 eta 1:04:04\n",
            "epoch [10/50] batch [380/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.2213 (0.4278) lr 2.3454e-03 eta 1:03:59\n",
            "epoch [10/50] batch [400/408] time 0.247 (0.235) data 0.017 (0.013) loss 0.4048 (0.4211) lr 2.3454e-03 eta 1:03:54\n",
            "epoch [11/50] batch [20/408] time 0.234 (0.235) data 0.015 (0.012) loss 0.2655 (0.3158) lr 2.3054e-03 eta 1:03:46\n",
            "epoch [11/50] batch [40/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2462 (0.3944) lr 2.3054e-03 eta 1:03:37\n",
            "epoch [11/50] batch [60/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2531 (0.4916) lr 2.3054e-03 eta 1:03:44\n",
            "epoch [11/50] batch [80/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2157 (0.4764) lr 2.3054e-03 eta 1:03:33\n",
            "epoch [11/50] batch [100/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2347 (0.4511) lr 2.3054e-03 eta 1:03:35\n",
            "epoch [11/50] batch [120/408] time 0.231 (0.235) data 0.009 (0.013) loss 0.2758 (0.4296) lr 2.3054e-03 eta 1:03:26\n",
            "epoch [11/50] batch [140/408] time 0.234 (0.235) data 0.010 (0.013) loss 0.2734 (0.4114) lr 2.3054e-03 eta 1:03:20\n",
            "epoch [11/50] batch [160/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3632 (0.4012) lr 2.3054e-03 eta 1:03:18\n",
            "epoch [11/50] batch [180/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.4102 (0.4013) lr 2.3054e-03 eta 1:03:11\n",
            "epoch [11/50] batch [200/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.4497 (0.3878) lr 2.3054e-03 eta 1:03:10\n",
            "epoch [11/50] batch [220/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.5903 (0.3950) lr 2.3054e-03 eta 1:03:04\n",
            "epoch [11/50] batch [240/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2660 (0.3996) lr 2.3054e-03 eta 1:02:58\n",
            "epoch [11/50] batch [260/408] time 0.230 (0.235) data 0.009 (0.013) loss 0.3091 (0.4056) lr 2.3054e-03 eta 1:02:55\n",
            "epoch [11/50] batch [280/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3467 (0.4143) lr 2.3054e-03 eta 1:02:48\n",
            "epoch [11/50] batch [300/408] time 0.246 (0.235) data 0.016 (0.013) loss 0.2980 (0.4203) lr 2.3054e-03 eta 1:02:46\n",
            "epoch [11/50] batch [320/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2114 (0.4272) lr 2.3054e-03 eta 1:02:40\n",
            "epoch [11/50] batch [340/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2913 (0.4249) lr 2.3054e-03 eta 1:02:34\n",
            "epoch [11/50] batch [360/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.8251 (0.4251) lr 2.3054e-03 eta 1:02:31\n",
            "epoch [11/50] batch [380/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2243 (0.4183) lr 2.3054e-03 eta 1:02:25\n",
            "epoch [11/50] batch [400/408] time 0.242 (0.235) data 0.021 (0.013) loss 0.1994 (0.4118) lr 2.3054e-03 eta 1:02:22\n",
            "epoch [12/50] batch [20/408] time 0.235 (0.234) data 0.012 (0.011) loss 0.2016 (0.3330) lr 2.2613e-03 eta 1:01:55\n",
            "epoch [12/50] batch [40/408] time 0.237 (0.235) data 0.017 (0.013) loss 0.2193 (0.3440) lr 2.2613e-03 eta 1:02:07\n",
            "epoch [12/50] batch [60/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.3958 (0.3752) lr 2.2613e-03 eta 1:02:03\n",
            "epoch [12/50] batch [80/408] time 0.234 (0.235) data 0.010 (0.012) loss 0.2484 (0.3797) lr 2.2613e-03 eta 1:01:53\n",
            "epoch [12/50] batch [100/408] time 0.231 (0.235) data 0.009 (0.013) loss 0.3041 (0.3820) lr 2.2613e-03 eta 1:01:53\n",
            "epoch [12/50] batch [120/408] time 0.236 (0.235) data 0.011 (0.012) loss 0.3156 (0.3791) lr 2.2613e-03 eta 1:01:47\n",
            "epoch [12/50] batch [140/408] time 0.240 (0.235) data 0.018 (0.012) loss 0.2812 (0.3846) lr 2.2613e-03 eta 1:01:45\n",
            "epoch [12/50] batch [160/408] time 0.235 (0.235) data 0.013 (0.012) loss 0.2214 (0.3789) lr 2.2613e-03 eta 1:01:41\n",
            "epoch [12/50] batch [180/408] time 0.231 (0.235) data 0.011 (0.012) loss 0.3429 (0.3901) lr 2.2613e-03 eta 1:01:35\n",
            "epoch [12/50] batch [200/408] time 0.239 (0.235) data 0.016 (0.013) loss 0.2400 (0.3819) lr 2.2613e-03 eta 1:01:32\n",
            "epoch [12/50] batch [220/408] time 0.235 (0.235) data 0.013 (0.012) loss 0.2268 (0.3794) lr 2.2613e-03 eta 1:01:26\n",
            "epoch [12/50] batch [240/408] time 0.241 (0.235) data 0.020 (0.013) loss 0.2302 (0.3866) lr 2.2613e-03 eta 1:01:22\n",
            "epoch [12/50] batch [260/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2560 (0.3844) lr 2.2613e-03 eta 1:01:18\n",
            "epoch [12/50] batch [280/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2426 (0.3754) lr 2.2613e-03 eta 1:01:12\n",
            "epoch [12/50] batch [300/408] time 0.238 (0.235) data 0.015 (0.013) loss 0.2263 (0.3673) lr 2.2613e-03 eta 1:01:10\n",
            "epoch [12/50] batch [320/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2316 (0.3645) lr 2.2613e-03 eta 1:01:04\n",
            "epoch [12/50] batch [340/408] time 0.245 (0.235) data 0.020 (0.013) loss 0.1962 (0.3707) lr 2.2613e-03 eta 1:01:01\n",
            "epoch [12/50] batch [360/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2032 (0.3686) lr 2.2613e-03 eta 1:00:57\n",
            "epoch [12/50] batch [380/408] time 0.230 (0.235) data 0.010 (0.013) loss 0.4398 (0.3735) lr 2.2613e-03 eta 1:00:51\n",
            "epoch [12/50] batch [400/408] time 0.233 (0.235) data 0.009 (0.013) loss 0.2943 (0.3769) lr 2.2613e-03 eta 1:00:48\n",
            "epoch [13/50] batch [20/408] time 0.235 (0.234) data 0.012 (0.011) loss 0.2484 (0.3093) lr 2.2131e-03 eta 1:00:24\n",
            "epoch [13/50] batch [40/408] time 0.233 (0.235) data 0.012 (0.012) loss 0.2142 (0.3614) lr 2.2131e-03 eta 1:00:34\n",
            "epoch [13/50] batch [60/408] time 0.233 (0.235) data 0.012 (0.012) loss 0.2359 (0.3620) lr 2.2131e-03 eta 1:00:25\n",
            "epoch [13/50] batch [80/408] time 0.240 (0.235) data 0.018 (0.012) loss 0.2338 (0.3490) lr 2.2131e-03 eta 1:00:19\n",
            "epoch [13/50] batch [100/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2653 (0.3621) lr 2.2131e-03 eta 1:00:19\n",
            "epoch [13/50] batch [120/408] time 0.234 (0.235) data 0.012 (0.012) loss 0.2104 (0.3775) lr 2.2131e-03 eta 1:00:11\n",
            "epoch [13/50] batch [140/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.9124 (0.3825) lr 2.2131e-03 eta 1:00:10\n",
            "epoch [13/50] batch [160/408] time 0.231 (0.235) data 0.011 (0.012) loss 0.6120 (0.3799) lr 2.2131e-03 eta 1:00:04\n",
            "epoch [13/50] batch [180/408] time 0.241 (0.235) data 0.017 (0.012) loss 0.2556 (0.3816) lr 2.2131e-03 eta 0:59:58\n",
            "epoch [13/50] batch [200/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.6124 (0.3757) lr 2.2131e-03 eta 0:59:55\n",
            "epoch [13/50] batch [220/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2049 (0.3778) lr 2.2131e-03 eta 0:59:49\n",
            "epoch [13/50] batch [240/408] time 0.232 (0.235) data 0.014 (0.013) loss 0.2105 (0.3723) lr 2.2131e-03 eta 0:59:46\n",
            "epoch [13/50] batch [260/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2051 (0.3646) lr 2.2131e-03 eta 0:59:39\n",
            "epoch [13/50] batch [280/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.5702 (0.3707) lr 2.2131e-03 eta 0:59:33\n",
            "epoch [13/50] batch [300/408] time 0.239 (0.235) data 0.011 (0.013) loss 0.3161 (0.3683) lr 2.2131e-03 eta 0:59:29\n",
            "epoch [13/50] batch [320/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2461 (0.3704) lr 2.2131e-03 eta 0:59:24\n",
            "epoch [13/50] batch [340/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2507 (0.3686) lr 2.2131e-03 eta 0:59:21\n",
            "epoch [13/50] batch [360/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.5984 (0.3827) lr 2.2131e-03 eta 0:59:15\n",
            "epoch [13/50] batch [380/408] time 0.238 (0.235) data 0.015 (0.013) loss 0.2784 (0.3812) lr 2.2131e-03 eta 0:59:10\n",
            "epoch [13/50] batch [400/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.9699 (0.3796) lr 2.2131e-03 eta 0:59:07\n",
            "epoch [14/50] batch [20/408] time 0.233 (0.235) data 0.013 (0.012) loss 0.2457 (0.2788) lr 2.1612e-03 eta 0:58:56\n",
            "epoch [14/50] batch [40/408] time 0.235 (0.236) data 0.013 (0.014) loss 0.2520 (0.3286) lr 2.1612e-03 eta 0:59:19\n",
            "epoch [14/50] batch [60/408] time 0.236 (0.236) data 0.016 (0.014) loss 0.6936 (0.3677) lr 2.1612e-03 eta 0:59:03\n",
            "epoch [14/50] batch [80/408] time 0.245 (0.236) data 0.020 (0.014) loss 0.3008 (0.3874) lr 2.1612e-03 eta 0:59:02\n",
            "epoch [14/50] batch [100/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.5259 (0.4007) lr 2.1612e-03 eta 0:58:50\n",
            "epoch [14/50] batch [120/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2663 (0.3876) lr 2.1612e-03 eta 0:58:41\n",
            "epoch [14/50] batch [140/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3311 (0.3940) lr 2.1612e-03 eta 0:58:38\n",
            "epoch [14/50] batch [160/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2360 (0.3869) lr 2.1612e-03 eta 0:58:29\n",
            "epoch [14/50] batch [180/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.4893 (0.3839) lr 2.1612e-03 eta 0:58:27\n",
            "epoch [14/50] batch [200/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2663 (0.3867) lr 2.1612e-03 eta 0:58:21\n",
            "epoch [14/50] batch [220/408] time 0.230 (0.235) data 0.009 (0.013) loss 2.3068 (0.3881) lr 2.1612e-03 eta 0:58:15\n",
            "epoch [14/50] batch [240/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2434 (0.3866) lr 2.1612e-03 eta 0:58:13\n",
            "epoch [14/50] batch [260/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2573 (0.3964) lr 2.1612e-03 eta 0:58:06\n",
            "epoch [14/50] batch [280/408] time 0.243 (0.235) data 0.017 (0.013) loss 2.3811 (0.4006) lr 2.1612e-03 eta 0:58:03\n",
            "epoch [14/50] batch [300/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3374 (0.4247) lr 2.1612e-03 eta 0:57:58\n",
            "epoch [14/50] batch [320/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2133 (0.4231) lr 2.1612e-03 eta 0:57:52\n",
            "epoch [14/50] batch [340/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.6702 (0.4139) lr 2.1612e-03 eta 0:57:50\n",
            "epoch [14/50] batch [360/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2586 (0.4091) lr 2.1612e-03 eta 0:57:45\n",
            "epoch [14/50] batch [380/408] time 0.239 (0.235) data 0.016 (0.013) loss 0.2958 (0.4035) lr 2.1612e-03 eta 0:57:40\n",
            "epoch [14/50] batch [400/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.4363 (0.3993) lr 2.1612e-03 eta 0:57:35\n",
            "epoch [15/50] batch [20/408] time 0.239 (0.235) data 0.018 (0.013) loss 0.2088 (0.4137) lr 2.1057e-03 eta 0:57:27\n",
            "epoch [15/50] batch [40/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.3695 (0.4724) lr 2.1057e-03 eta 0:57:29\n",
            "epoch [15/50] batch [60/408] time 0.232 (0.235) data 0.012 (0.012) loss 1.8443 (0.4885) lr 2.1057e-03 eta 0:57:14\n",
            "epoch [15/50] batch [80/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2624 (0.4710) lr 2.1057e-03 eta 0:57:16\n",
            "epoch [15/50] batch [100/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.4270 (0.4806) lr 2.1057e-03 eta 0:57:06\n",
            "epoch [15/50] batch [120/408] time 0.237 (0.235) data 0.016 (0.013) loss 0.3361 (0.4648) lr 2.1057e-03 eta 0:56:59\n",
            "epoch [15/50] batch [140/408] time 0.235 (0.235) data 0.013 (0.013) loss 1.8404 (0.4697) lr 2.1057e-03 eta 0:56:55\n",
            "epoch [15/50] batch [160/408] time 0.233 (0.235) data 0.013 (0.012) loss 0.2414 (0.4477) lr 2.1057e-03 eta 0:56:49\n",
            "epoch [15/50] batch [180/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2252 (0.4629) lr 2.1057e-03 eta 0:56:47\n",
            "epoch [15/50] batch [200/408] time 0.236 (0.235) data 0.012 (0.012) loss 0.5530 (0.4617) lr 2.1057e-03 eta 0:56:40\n",
            "epoch [15/50] batch [220/408] time 0.240 (0.235) data 0.017 (0.012) loss 0.2565 (0.4524) lr 2.1057e-03 eta 0:56:36\n",
            "epoch [15/50] batch [240/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2001 (0.4454) lr 2.1057e-03 eta 0:56:32\n",
            "epoch [15/50] batch [260/408] time 0.230 (0.235) data 0.012 (0.012) loss 0.4038 (0.4384) lr 2.1057e-03 eta 0:56:25\n",
            "epoch [15/50] batch [280/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.1922 (0.4305) lr 2.1057e-03 eta 0:56:22\n",
            "epoch [15/50] batch [300/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2305 (0.4207) lr 2.1057e-03 eta 0:56:17\n",
            "epoch [15/50] batch [320/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.3831 (0.4217) lr 2.1057e-03 eta 0:56:12\n",
            "epoch [15/50] batch [340/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2450 (0.4163) lr 2.1057e-03 eta 0:56:08\n",
            "epoch [15/50] batch [360/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.4307 (0.4177) lr 2.1057e-03 eta 0:56:02\n",
            "epoch [15/50] batch [380/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2432 (0.4155) lr 2.1057e-03 eta 0:55:59\n",
            "epoch [15/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2387 (0.4125) lr 2.1057e-03 eta 0:55:54\n",
            "epoch [16/50] batch [20/408] time 0.234 (0.236) data 0.012 (0.014) loss 0.2516 (0.4497) lr 2.0468e-03 eta 0:56:08\n",
            "epoch [16/50] batch [40/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2043 (0.4150) lr 2.0468e-03 eta 0:55:50\n",
            "epoch [16/50] batch [60/408] time 0.241 (0.235) data 0.015 (0.013) loss 0.2278 (0.4587) lr 2.0468e-03 eta 0:55:42\n",
            "epoch [16/50] batch [80/408] time 0.229 (0.235) data 0.010 (0.013) loss 0.2554 (0.4319) lr 2.0468e-03 eta 0:55:42\n",
            "epoch [16/50] batch [100/408] time 0.236 (0.235) data 0.011 (0.013) loss 0.2025 (0.4115) lr 2.0468e-03 eta 0:55:33\n",
            "epoch [16/50] batch [120/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.2296 (0.4057) lr 2.0468e-03 eta 0:55:31\n",
            "epoch [16/50] batch [140/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.3901 (0.3918) lr 2.0468e-03 eta 0:55:23\n",
            "epoch [16/50] batch [160/408] time 0.236 (0.235) data 0.011 (0.012) loss 0.3017 (0.4422) lr 2.0468e-03 eta 0:55:15\n",
            "epoch [16/50] batch [180/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.3856 (0.4363) lr 2.0468e-03 eta 0:55:12\n",
            "epoch [16/50] batch [200/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3218 (0.4311) lr 2.0468e-03 eta 0:55:06\n",
            "epoch [16/50] batch [220/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3719 (0.4439) lr 2.0468e-03 eta 0:55:03\n",
            "epoch [16/50] batch [240/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2246 (0.4451) lr 2.0468e-03 eta 0:54:56\n",
            "epoch [16/50] batch [260/408] time 0.236 (0.235) data 0.011 (0.012) loss 0.2427 (0.4327) lr 2.0468e-03 eta 0:54:50\n",
            "epoch [16/50] batch [280/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2611 (0.4277) lr 2.0468e-03 eta 0:54:47\n",
            "epoch [16/50] batch [300/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2051 (0.4209) lr 2.0468e-03 eta 0:54:41\n",
            "epoch [16/50] batch [320/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3339 (0.4324) lr 2.0468e-03 eta 0:54:37\n",
            "epoch [16/50] batch [340/408] time 0.232 (0.235) data 0.013 (0.013) loss 0.2989 (0.4275) lr 2.0468e-03 eta 0:54:31\n",
            "epoch [16/50] batch [360/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2828 (0.4307) lr 2.0468e-03 eta 0:54:26\n",
            "epoch [16/50] batch [380/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2359 (0.4368) lr 2.0468e-03 eta 0:54:22\n",
            "epoch [16/50] batch [400/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.3162 (0.4334) lr 2.0468e-03 eta 0:54:16\n",
            "epoch [17/50] batch [20/408] time 0.232 (0.236) data 0.010 (0.014) loss 0.2174 (0.3560) lr 1.9847e-03 eta 0:54:29\n",
            "epoch [17/50] batch [40/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2280 (0.3541) lr 1.9847e-03 eta 0:54:11\n",
            "epoch [17/50] batch [60/408] time 0.241 (0.235) data 0.017 (0.013) loss 0.2497 (0.3490) lr 1.9847e-03 eta 0:54:11\n",
            "epoch [17/50] batch [80/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3261 (0.3477) lr 1.9847e-03 eta 0:54:02\n",
            "epoch [17/50] batch [100/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2341 (0.3770) lr 1.9847e-03 eta 0:53:53\n",
            "epoch [17/50] batch [120/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.3332 (0.3703) lr 1.9847e-03 eta 0:53:52\n",
            "epoch [17/50] batch [140/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2093 (0.3824) lr 1.9847e-03 eta 0:53:45\n",
            "epoch [17/50] batch [160/408] time 0.239 (0.235) data 0.020 (0.013) loss 0.2561 (0.3760) lr 1.9847e-03 eta 0:53:42\n",
            "epoch [17/50] batch [180/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2789 (0.3812) lr 1.9847e-03 eta 0:53:35\n",
            "epoch [17/50] batch [200/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2503 (0.3775) lr 1.9847e-03 eta 0:53:28\n",
            "epoch [17/50] batch [220/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2147 (0.3916) lr 1.9847e-03 eta 0:53:26\n",
            "epoch [17/50] batch [240/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2433 (0.3845) lr 1.9847e-03 eta 0:53:20\n",
            "epoch [17/50] batch [260/408] time 0.241 (0.235) data 0.017 (0.013) loss 0.1994 (0.3848) lr 1.9847e-03 eta 0:53:16\n",
            "epoch [17/50] batch [280/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.3387 (0.3836) lr 1.9847e-03 eta 0:53:12\n",
            "epoch [17/50] batch [300/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2839 (0.3793) lr 1.9847e-03 eta 0:53:07\n",
            "epoch [17/50] batch [320/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2308 (0.3728) lr 1.9847e-03 eta 0:53:04\n",
            "epoch [17/50] batch [340/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2250 (0.3687) lr 1.9847e-03 eta 0:52:59\n",
            "epoch [17/50] batch [360/408] time 0.247 (0.235) data 0.019 (0.013) loss 0.4102 (0.3792) lr 1.9847e-03 eta 0:52:56\n",
            "epoch [17/50] batch [380/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2767 (0.3779) lr 1.9847e-03 eta 0:52:52\n",
            "epoch [17/50] batch [400/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2579 (0.3775) lr 1.9847e-03 eta 0:52:47\n",
            "epoch [18/50] batch [20/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.2385 (0.3812) lr 1.9198e-03 eta 0:52:52\n",
            "epoch [18/50] batch [40/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.2140 (0.4039) lr 1.9198e-03 eta 0:52:34\n",
            "epoch [18/50] batch [60/408] time 0.232 (0.236) data 0.011 (0.013) loss 0.2336 (0.3973) lr 1.9198e-03 eta 0:52:39\n",
            "epoch [18/50] batch [80/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.3003 (0.3881) lr 1.9198e-03 eta 0:52:31\n",
            "epoch [18/50] batch [100/408] time 0.240 (0.236) data 0.016 (0.013) loss 0.4532 (0.3800) lr 1.9198e-03 eta 0:52:27\n",
            "epoch [18/50] batch [120/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.1907 (0.3835) lr 1.9198e-03 eta 0:52:23\n",
            "epoch [18/50] batch [140/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.1986 (0.3753) lr 1.9198e-03 eta 0:52:15\n",
            "epoch [18/50] batch [160/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.2374 (0.3819) lr 1.9198e-03 eta 0:52:13\n",
            "epoch [18/50] batch [180/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2357 (0.3756) lr 1.9198e-03 eta 0:52:06\n",
            "epoch [18/50] batch [200/408] time 0.238 (0.235) data 0.016 (0.013) loss 0.4066 (0.3793) lr 1.9198e-03 eta 0:52:01\n",
            "epoch [18/50] batch [220/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2763 (0.3718) lr 1.9198e-03 eta 0:51:55\n",
            "epoch [18/50] batch [240/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2153 (0.3675) lr 1.9198e-03 eta 0:51:49\n",
            "epoch [18/50] batch [260/408] time 0.234 (0.235) data 0.010 (0.013) loss 0.2460 (0.3732) lr 1.9198e-03 eta 0:51:46\n",
            "epoch [18/50] batch [280/408] time 0.231 (0.235) data 0.009 (0.013) loss 0.3113 (0.3637) lr 1.9198e-03 eta 0:51:40\n",
            "epoch [18/50] batch [300/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.1957 (0.3569) lr 1.9198e-03 eta 0:51:35\n",
            "epoch [18/50] batch [320/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.4869 (0.3541) lr 1.9198e-03 eta 0:51:31\n",
            "epoch [18/50] batch [340/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2028 (0.3536) lr 1.9198e-03 eta 0:51:25\n",
            "epoch [18/50] batch [360/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2004 (0.3515) lr 1.9198e-03 eta 0:51:21\n",
            "epoch [18/50] batch [380/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2742 (0.3481) lr 1.9198e-03 eta 0:51:16\n",
            "epoch [18/50] batch [400/408] time 0.234 (0.235) data 0.016 (0.013) loss 0.2071 (0.3456) lr 1.9198e-03 eta 0:51:12\n",
            "epoch [19/50] batch [20/408] time 0.239 (0.234) data 0.016 (0.011) loss 0.4002 (0.4998) lr 1.8522e-03 eta 0:50:54\n",
            "epoch [19/50] batch [40/408] time 0.242 (0.234) data 0.019 (0.012) loss 0.2758 (0.4234) lr 1.8522e-03 eta 0:50:51\n",
            "epoch [19/50] batch [60/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2167 (0.4493) lr 1.8522e-03 eta 0:50:54\n",
            "epoch [19/50] batch [80/408] time 0.233 (0.235) data 0.012 (0.012) loss 0.2246 (0.4382) lr 1.8522e-03 eta 0:50:46\n",
            "epoch [19/50] batch [100/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2481 (0.4162) lr 1.8522e-03 eta 0:50:47\n",
            "epoch [19/50] batch [120/408] time 0.236 (0.235) data 0.012 (0.013) loss 2.4413 (0.4149) lr 1.8522e-03 eta 0:50:42\n",
            "epoch [19/50] batch [140/408] time 0.242 (0.235) data 0.021 (0.013) loss 0.2492 (0.4248) lr 1.8522e-03 eta 0:50:36\n",
            "epoch [19/50] batch [160/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.2924 (0.4080) lr 1.8522e-03 eta 0:50:33\n",
            "epoch [19/50] batch [180/408] time 0.235 (0.235) data 0.010 (0.013) loss 0.9864 (0.4077) lr 1.8522e-03 eta 0:50:27\n",
            "epoch [19/50] batch [200/408] time 0.234 (0.235) data 0.014 (0.013) loss 0.2951 (0.4116) lr 1.8522e-03 eta 0:50:23\n",
            "epoch [19/50] batch [220/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2652 (0.4206) lr 1.8522e-03 eta 0:50:17\n",
            "epoch [19/50] batch [240/408] time 0.241 (0.235) data 0.014 (0.013) loss 0.2497 (0.4198) lr 1.8522e-03 eta 0:50:11\n",
            "epoch [19/50] batch [260/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.8698 (0.4240) lr 1.8522e-03 eta 0:50:07\n",
            "epoch [19/50] batch [280/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.3834 (0.4180) lr 1.8522e-03 eta 0:50:01\n",
            "epoch [19/50] batch [300/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2587 (0.4141) lr 1.8522e-03 eta 0:49:57\n",
            "epoch [19/50] batch [320/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1995 (0.4079) lr 1.8522e-03 eta 0:49:52\n",
            "epoch [19/50] batch [340/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2572 (0.4098) lr 1.8522e-03 eta 0:49:46\n",
            "epoch [19/50] batch [360/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.1973 (0.4070) lr 1.8522e-03 eta 0:49:43\n",
            "epoch [19/50] batch [380/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2235 (0.4029) lr 1.8522e-03 eta 0:49:37\n",
            "epoch [19/50] batch [400/408] time 0.232 (0.235) data 0.010 (0.013) loss 1.0676 (0.4063) lr 1.8522e-03 eta 0:49:34\n",
            "epoch [20/50] batch [20/408] time 0.236 (0.234) data 0.011 (0.012) loss 0.2160 (0.3941) lr 1.7822e-03 eta 0:49:21\n",
            "epoch [20/50] batch [40/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.1991 (0.3347) lr 1.7822e-03 eta 0:49:31\n",
            "epoch [20/50] batch [60/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2831 (0.4363) lr 1.7822e-03 eta 0:49:20\n",
            "epoch [20/50] batch [80/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.3405 (0.3980) lr 1.7822e-03 eta 0:49:12\n",
            "epoch [20/50] batch [100/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1993 (0.3805) lr 1.7822e-03 eta 0:49:10\n",
            "epoch [20/50] batch [120/408] time 0.232 (0.235) data 0.013 (0.013) loss 0.2297 (0.3682) lr 1.7822e-03 eta 0:49:04\n",
            "epoch [20/50] batch [140/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.1954 (0.3499) lr 1.7822e-03 eta 0:49:02\n",
            "epoch [20/50] batch [160/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.7174 (0.3543) lr 1.7822e-03 eta 0:48:56\n",
            "epoch [20/50] batch [180/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1946 (0.3516) lr 1.7822e-03 eta 0:48:50\n",
            "epoch [20/50] batch [200/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2367 (0.3608) lr 1.7822e-03 eta 0:48:48\n",
            "epoch [20/50] batch [220/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2208 (0.3514) lr 1.7822e-03 eta 0:48:42\n",
            "epoch [20/50] batch [240/408] time 0.239 (0.235) data 0.016 (0.013) loss 0.5400 (0.3631) lr 1.7822e-03 eta 0:48:38\n",
            "epoch [20/50] batch [260/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2540 (0.3617) lr 1.7822e-03 eta 0:48:32\n",
            "epoch [20/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2062 (0.3619) lr 1.7822e-03 eta 0:48:26\n",
            "epoch [20/50] batch [300/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2021 (0.3579) lr 1.7822e-03 eta 0:48:24\n",
            "epoch [20/50] batch [320/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2904 (0.3511) lr 1.7822e-03 eta 0:48:18\n",
            "epoch [20/50] batch [340/408] time 0.240 (0.235) data 0.016 (0.013) loss 0.2456 (0.3476) lr 1.7822e-03 eta 0:48:14\n",
            "epoch [20/50] batch [360/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2249 (0.3435) lr 1.7822e-03 eta 0:48:09\n",
            "epoch [20/50] batch [380/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2197 (0.3402) lr 1.7822e-03 eta 0:48:03\n",
            "epoch [20/50] batch [400/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.1906 (0.3406) lr 1.7822e-03 eta 0:47:59\n",
            "epoch [21/50] batch [20/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.3827 (0.2686) lr 1.7102e-03 eta 0:47:40\n",
            "epoch [21/50] batch [40/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.7041 (0.3224) lr 1.7102e-03 eta 0:47:48\n",
            "epoch [21/50] batch [60/408] time 0.232 (0.235) data 0.012 (0.012) loss 0.1921 (0.3101) lr 1.7102e-03 eta 0:47:37\n",
            "epoch [21/50] batch [80/408] time 0.239 (0.234) data 0.017 (0.012) loss 0.2138 (0.3039) lr 1.7102e-03 eta 0:47:30\n",
            "epoch [21/50] batch [100/408] time 0.231 (0.234) data 0.010 (0.012) loss 0.2368 (0.3264) lr 1.7102e-03 eta 0:47:26\n",
            "epoch [21/50] batch [120/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.1956 (0.3241) lr 1.7102e-03 eta 0:47:21\n",
            "epoch [21/50] batch [140/408] time 0.235 (0.235) data 0.010 (0.012) loss 2.1647 (0.3364) lr 1.7102e-03 eta 0:47:19\n",
            "epoch [21/50] batch [160/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.3389 (0.3411) lr 1.7102e-03 eta 0:47:13\n",
            "epoch [21/50] batch [180/408] time 0.243 (0.235) data 0.018 (0.012) loss 0.2127 (0.3450) lr 1.7102e-03 eta 0:47:08\n",
            "epoch [21/50] batch [200/408] time 0.233 (0.235) data 0.013 (0.012) loss 0.2423 (0.3441) lr 1.7102e-03 eta 0:47:03\n",
            "epoch [21/50] batch [220/408] time 0.235 (0.235) data 0.013 (0.012) loss 0.2553 (0.3521) lr 1.7102e-03 eta 0:46:58\n",
            "epoch [21/50] batch [240/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2207 (0.3489) lr 1.7102e-03 eta 0:46:55\n",
            "epoch [21/50] batch [260/408] time 0.235 (0.235) data 0.012 (0.012) loss 0.2881 (0.3439) lr 1.7102e-03 eta 0:46:50\n",
            "epoch [21/50] batch [280/408] time 0.240 (0.235) data 0.018 (0.013) loss 0.2096 (0.3376) lr 1.7102e-03 eta 0:46:45\n",
            "epoch [21/50] batch [300/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2056 (0.3353) lr 1.7102e-03 eta 0:46:41\n",
            "epoch [21/50] batch [320/408] time 0.238 (0.235) data 0.016 (0.013) loss 0.1901 (0.3410) lr 1.7102e-03 eta 0:46:36\n",
            "epoch [21/50] batch [340/408] time 0.232 (0.235) data 0.013 (0.013) loss 0.1866 (0.3460) lr 1.7102e-03 eta 0:46:33\n",
            "epoch [21/50] batch [360/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2130 (0.3499) lr 1.7102e-03 eta 0:46:28\n",
            "epoch [21/50] batch [380/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.2380 (0.3548) lr 1.7102e-03 eta 0:46:24\n",
            "epoch [21/50] batch [400/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2518 (0.3591) lr 1.7102e-03 eta 0:46:19\n",
            "epoch [22/50] batch [20/408] time 0.240 (0.235) data 0.018 (0.012) loss 0.2057 (0.3316) lr 1.6363e-03 eta 0:46:09\n",
            "epoch [22/50] batch [40/408] time 0.234 (0.235) data 0.011 (0.013) loss 2.6097 (0.3898) lr 1.6363e-03 eta 0:46:15\n",
            "epoch [22/50] batch [60/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2031 (0.4018) lr 1.6363e-03 eta 0:46:04\n",
            "epoch [22/50] batch [80/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.9081 (0.4313) lr 1.6363e-03 eta 0:46:04\n",
            "epoch [22/50] batch [100/408] time 0.234 (0.235) data 0.014 (0.013) loss 0.2027 (0.3964) lr 1.6363e-03 eta 0:45:57\n",
            "epoch [22/50] batch [120/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2093 (0.3879) lr 1.6363e-03 eta 0:45:49\n",
            "epoch [22/50] batch [140/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2347 (0.3987) lr 1.6363e-03 eta 0:45:46\n",
            "epoch [22/50] batch [160/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2061 (0.3901) lr 1.6363e-03 eta 0:45:39\n",
            "epoch [22/50] batch [180/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.4404 (0.3982) lr 1.6363e-03 eta 0:45:37\n",
            "epoch [22/50] batch [200/408] time 0.240 (0.235) data 0.013 (0.013) loss 0.2529 (0.4142) lr 1.6363e-03 eta 0:45:31\n",
            "epoch [22/50] batch [220/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2204 (0.4027) lr 1.6363e-03 eta 0:45:25\n",
            "epoch [22/50] batch [240/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.3180 (0.3980) lr 1.6363e-03 eta 0:45:22\n",
            "epoch [22/50] batch [260/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.1982 (0.3921) lr 1.6363e-03 eta 0:45:16\n",
            "epoch [22/50] batch [280/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2282 (0.4024) lr 1.6363e-03 eta 0:45:13\n",
            "epoch [22/50] batch [300/408] time 0.230 (0.235) data 0.009 (0.013) loss 0.3650 (0.3989) lr 1.6363e-03 eta 0:45:08\n",
            "epoch [22/50] batch [320/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.5307 (0.3950) lr 1.6363e-03 eta 0:45:03\n",
            "epoch [22/50] batch [340/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2117 (0.4004) lr 1.6363e-03 eta 0:45:00\n",
            "epoch [22/50] batch [360/408] time 0.228 (0.235) data 0.009 (0.013) loss 1.0308 (0.3991) lr 1.6363e-03 eta 0:44:54\n",
            "epoch [22/50] batch [380/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.2749 (0.3972) lr 1.6363e-03 eta 0:44:49\n",
            "epoch [22/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2930 (0.3918) lr 1.6363e-03 eta 0:44:44\n",
            "epoch [23/50] batch [20/408] time 0.247 (0.236) data 0.025 (0.014) loss 0.2544 (0.3527) lr 1.5609e-03 eta 0:44:56\n",
            "epoch [23/50] batch [40/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.3105 (0.3581) lr 1.5609e-03 eta 0:44:32\n",
            "epoch [23/50] batch [60/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2410 (0.3748) lr 1.5609e-03 eta 0:44:25\n",
            "epoch [23/50] batch [80/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.2124 (0.3713) lr 1.5609e-03 eta 0:44:33\n",
            "epoch [23/50] batch [100/408] time 0.238 (0.235) data 0.014 (0.013) loss 0.2500 (0.3669) lr 1.5609e-03 eta 0:44:24\n",
            "epoch [23/50] batch [120/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.4061 (0.3562) lr 1.5609e-03 eta 0:44:22\n",
            "epoch [23/50] batch [140/408] time 0.237 (0.235) data 0.011 (0.013) loss 0.2189 (0.3465) lr 1.5609e-03 eta 0:44:17\n",
            "epoch [23/50] batch [160/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2365 (0.3465) lr 1.5609e-03 eta 0:44:10\n",
            "epoch [23/50] batch [180/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3794 (0.3426) lr 1.5609e-03 eta 0:44:07\n",
            "epoch [23/50] batch [200/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2701 (0.3449) lr 1.5609e-03 eta 0:44:00\n",
            "epoch [23/50] batch [220/408] time 0.240 (0.235) data 0.019 (0.013) loss 0.2086 (0.3560) lr 1.5609e-03 eta 0:43:56\n",
            "epoch [23/50] batch [240/408] time 0.238 (0.235) data 0.012 (0.013) loss 0.2676 (0.3561) lr 1.5609e-03 eta 0:43:52\n",
            "epoch [23/50] batch [260/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.3262 (0.3599) lr 1.5609e-03 eta 0:43:46\n",
            "epoch [23/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2912 (0.3613) lr 1.5609e-03 eta 0:43:43\n",
            "epoch [23/50] batch [300/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2986 (0.3648) lr 1.5609e-03 eta 0:43:38\n",
            "epoch [23/50] batch [320/408] time 0.237 (0.235) data 0.016 (0.013) loss 0.3181 (0.3592) lr 1.5609e-03 eta 0:43:33\n",
            "epoch [23/50] batch [340/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2852 (0.3550) lr 1.5609e-03 eta 0:43:29\n",
            "epoch [23/50] batch [360/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2611 (0.3601) lr 1.5609e-03 eta 0:43:24\n",
            "epoch [23/50] batch [380/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2639 (0.3547) lr 1.5609e-03 eta 0:43:19\n",
            "epoch [23/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 1.3693 (0.3587) lr 1.5609e-03 eta 0:43:15\n",
            "epoch [24/50] batch [20/408] time 0.239 (0.239) data 0.013 (0.015) loss 0.2347 (0.3821) lr 1.4842e-03 eta 0:43:48\n",
            "epoch [24/50] batch [40/408] time 0.231 (0.237) data 0.010 (0.013) loss 1.2762 (0.3647) lr 1.4842e-03 eta 0:43:18\n",
            "epoch [24/50] batch [60/408] time 0.237 (0.236) data 0.013 (0.013) loss 0.2193 (0.3372) lr 1.4842e-03 eta 0:43:09\n",
            "epoch [24/50] batch [80/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.1878 (0.3430) lr 1.4842e-03 eta 0:43:03\n",
            "epoch [24/50] batch [100/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.1773 (0.3267) lr 1.4842e-03 eta 0:42:53\n",
            "epoch [24/50] batch [120/408] time 0.237 (0.236) data 0.014 (0.013) loss 0.3825 (0.3456) lr 1.4842e-03 eta 0:42:51\n",
            "epoch [24/50] batch [140/408] time 0.234 (0.236) data 0.011 (0.013) loss 0.2716 (0.3361) lr 1.4842e-03 eta 0:42:45\n",
            "epoch [24/50] batch [160/408] time 0.239 (0.236) data 0.018 (0.013) loss 0.2281 (0.3349) lr 1.4842e-03 eta 0:42:38\n",
            "epoch [24/50] batch [180/408] time 0.235 (0.236) data 0.011 (0.013) loss 0.2980 (0.3365) lr 1.4842e-03 eta 0:42:36\n",
            "epoch [24/50] batch [200/408] time 0.232 (0.236) data 0.010 (0.013) loss 0.2668 (0.3396) lr 1.4842e-03 eta 0:42:29\n",
            "epoch [24/50] batch [220/408] time 0.234 (0.236) data 0.011 (0.013) loss 0.1997 (0.3468) lr 1.4842e-03 eta 0:42:26\n",
            "epoch [24/50] batch [240/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.2505 (0.3499) lr 1.4842e-03 eta 0:42:20\n",
            "epoch [24/50] batch [260/408] time 0.240 (0.236) data 0.018 (0.013) loss 0.2230 (0.3590) lr 1.4842e-03 eta 0:42:15\n",
            "epoch [24/50] batch [280/408] time 0.234 (0.236) data 0.013 (0.013) loss 0.1892 (0.3618) lr 1.4842e-03 eta 0:42:12\n",
            "epoch [24/50] batch [300/408] time 0.230 (0.236) data 0.011 (0.013) loss 0.2654 (0.3590) lr 1.4842e-03 eta 0:42:06\n",
            "epoch [24/50] batch [320/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.2410 (0.3523) lr 1.4842e-03 eta 0:42:02\n",
            "epoch [24/50] batch [340/408] time 0.233 (0.236) data 0.014 (0.013) loss 0.1852 (0.3558) lr 1.4842e-03 eta 0:41:56\n",
            "epoch [24/50] batch [360/408] time 0.240 (0.236) data 0.013 (0.013) loss 0.2145 (0.3607) lr 1.4842e-03 eta 0:41:50\n",
            "epoch [24/50] batch [380/408] time 0.232 (0.236) data 0.010 (0.013) loss 2.0397 (0.3692) lr 1.4842e-03 eta 0:41:46\n",
            "epoch [24/50] batch [400/408] time 0.235 (0.236) data 0.010 (0.013) loss 0.2713 (0.3662) lr 1.4842e-03 eta 0:41:40\n",
            "epoch [25/50] batch [20/408] time 0.233 (0.237) data 0.011 (0.014) loss 0.2798 (0.3820) lr 1.4067e-03 eta 0:41:47\n",
            "epoch [25/50] batch [40/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1841 (0.3831) lr 1.4067e-03 eta 0:41:26\n",
            "epoch [25/50] batch [60/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.8328 (0.3607) lr 1.4067e-03 eta 0:41:22\n",
            "epoch [25/50] batch [80/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2029 (0.3652) lr 1.4067e-03 eta 0:41:15\n",
            "epoch [25/50] batch [100/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.3380 (0.3472) lr 1.4067e-03 eta 0:41:08\n",
            "epoch [25/50] batch [120/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2353 (0.3479) lr 1.4067e-03 eta 0:41:04\n",
            "epoch [25/50] batch [140/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1916 (0.3408) lr 1.4067e-03 eta 0:40:57\n",
            "epoch [25/50] batch [160/408] time 0.246 (0.235) data 0.020 (0.013) loss 0.1993 (0.3480) lr 1.4067e-03 eta 0:40:54\n",
            "epoch [25/50] batch [180/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.4166 (0.3599) lr 1.4067e-03 eta 0:40:49\n",
            "epoch [25/50] batch [200/408] time 0.230 (0.235) data 0.009 (0.013) loss 0.4144 (0.3743) lr 1.4067e-03 eta 0:40:44\n",
            "epoch [25/50] batch [220/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2220 (0.3676) lr 1.4067e-03 eta 0:40:42\n",
            "epoch [25/50] batch [240/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2032 (0.3635) lr 1.4067e-03 eta 0:40:37\n",
            "epoch [25/50] batch [260/408] time 0.238 (0.235) data 0.016 (0.013) loss 0.3895 (0.3617) lr 1.4067e-03 eta 0:40:33\n",
            "epoch [25/50] batch [280/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.3060 (0.3707) lr 1.4067e-03 eta 0:40:27\n",
            "epoch [25/50] batch [300/408] time 0.233 (0.235) data 0.015 (0.013) loss 0.1996 (0.3692) lr 1.4067e-03 eta 0:40:22\n",
            "epoch [25/50] batch [320/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2903 (0.3649) lr 1.4067e-03 eta 0:40:18\n",
            "epoch [25/50] batch [340/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2387 (0.3645) lr 1.4067e-03 eta 0:40:13\n",
            "epoch [25/50] batch [360/408] time 0.241 (0.235) data 0.018 (0.013) loss 0.2041 (0.3640) lr 1.4067e-03 eta 0:40:09\n",
            "epoch [25/50] batch [380/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1949 (0.3649) lr 1.4067e-03 eta 0:40:04\n",
            "epoch [25/50] batch [400/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.1980 (0.3671) lr 1.4067e-03 eta 0:39:59\n",
            "epoch [26/50] batch [20/408] time 0.234 (0.235) data 0.014 (0.013) loss 0.3291 (0.2727) lr 1.3285e-03 eta 0:39:51\n",
            "epoch [26/50] batch [40/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1910 (0.3084) lr 1.3285e-03 eta 0:39:46\n",
            "epoch [26/50] batch [60/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.3921 (0.2877) lr 1.3285e-03 eta 0:39:51\n",
            "epoch [26/50] batch [80/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2805 (0.3164) lr 1.3285e-03 eta 0:39:42\n",
            "epoch [26/50] batch [100/408] time 0.238 (0.235) data 0.015 (0.013) loss 0.7142 (0.3849) lr 1.3285e-03 eta 0:39:38\n",
            "epoch [26/50] batch [120/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.1849 (0.3672) lr 1.3285e-03 eta 0:39:32\n",
            "epoch [26/50] batch [140/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2621 (0.3668) lr 1.3285e-03 eta 0:39:26\n",
            "epoch [26/50] batch [160/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.2780 (0.3591) lr 1.3285e-03 eta 0:39:22\n",
            "epoch [26/50] batch [180/408] time 0.238 (0.235) data 0.015 (0.013) loss 0.2330 (0.3470) lr 1.3285e-03 eta 0:39:16\n",
            "epoch [26/50] batch [200/408] time 0.236 (0.235) data 0.015 (0.013) loss 0.3521 (0.3506) lr 1.3285e-03 eta 0:39:11\n",
            "epoch [26/50] batch [220/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2476 (0.3456) lr 1.3285e-03 eta 0:39:05\n",
            "epoch [26/50] batch [240/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.2396 (0.3529) lr 1.3285e-03 eta 0:38:59\n",
            "epoch [26/50] batch [260/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.6098 (0.3561) lr 1.3285e-03 eta 0:38:55\n",
            "epoch [26/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1918 (0.3501) lr 1.3285e-03 eta 0:38:49\n",
            "epoch [26/50] batch [300/408] time 0.242 (0.235) data 0.022 (0.013) loss 0.3929 (0.3426) lr 1.3285e-03 eta 0:38:45\n",
            "epoch [26/50] batch [320/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.1883 (0.3410) lr 1.3285e-03 eta 0:38:40\n",
            "epoch [26/50] batch [340/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.4782 (0.3471) lr 1.3285e-03 eta 0:38:34\n",
            "epoch [26/50] batch [360/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2393 (0.3444) lr 1.3285e-03 eta 0:38:30\n",
            "epoch [26/50] batch [380/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2292 (0.3459) lr 1.3285e-03 eta 0:38:25\n",
            "epoch [26/50] batch [400/408] time 0.238 (0.235) data 0.013 (0.013) loss 2.2963 (0.3529) lr 1.3285e-03 eta 0:38:20\n",
            "epoch [27/50] batch [20/408] time 0.232 (0.234) data 0.012 (0.012) loss 0.2795 (0.2798) lr 1.2500e-03 eta 0:38:04\n",
            "epoch [27/50] batch [40/408] time 0.241 (0.233) data 0.018 (0.012) loss 0.5493 (0.3556) lr 1.2500e-03 eta 0:37:56\n",
            "epoch [27/50] batch [60/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.2134 (0.3609) lr 1.2500e-03 eta 0:37:55\n",
            "epoch [27/50] batch [80/408] time 0.234 (0.234) data 0.013 (0.012) loss 0.2202 (0.3409) lr 1.2500e-03 eta 0:37:50\n",
            "epoch [27/50] batch [100/408] time 0.230 (0.234) data 0.011 (0.012) loss 0.1864 (0.3359) lr 1.2500e-03 eta 0:37:48\n",
            "epoch [27/50] batch [120/408] time 0.235 (0.234) data 0.013 (0.012) loss 0.2149 (0.3280) lr 1.2500e-03 eta 0:37:43\n",
            "epoch [27/50] batch [140/408] time 0.242 (0.234) data 0.018 (0.012) loss 0.1871 (0.3407) lr 1.2500e-03 eta 0:37:38\n",
            "epoch [27/50] batch [160/408] time 0.233 (0.234) data 0.011 (0.012) loss 0.8543 (0.3500) lr 1.2500e-03 eta 0:37:36\n",
            "epoch [27/50] batch [180/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.8262 (0.3522) lr 1.2500e-03 eta 0:37:30\n",
            "epoch [27/50] batch [200/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.1895 (0.3476) lr 1.2500e-03 eta 0:37:28\n",
            "epoch [27/50] batch [220/408] time 0.233 (0.234) data 0.010 (0.012) loss 1.1969 (0.3433) lr 1.2500e-03 eta 0:37:23\n",
            "epoch [27/50] batch [240/408] time 0.235 (0.234) data 0.011 (0.012) loss 0.6519 (0.3418) lr 1.2500e-03 eta 0:37:18\n",
            "epoch [27/50] batch [260/408] time 0.234 (0.234) data 0.011 (0.012) loss 0.2616 (0.3349) lr 1.2500e-03 eta 0:37:14\n",
            "epoch [27/50] batch [280/408] time 0.233 (0.234) data 0.011 (0.012) loss 0.2591 (0.3325) lr 1.2500e-03 eta 0:37:09\n",
            "epoch [27/50] batch [300/408] time 0.237 (0.234) data 0.012 (0.012) loss 0.6553 (0.3306) lr 1.2500e-03 eta 0:37:05\n",
            "epoch [27/50] batch [320/408] time 0.234 (0.234) data 0.010 (0.012) loss 3.3166 (0.3428) lr 1.2500e-03 eta 0:37:00\n",
            "epoch [27/50] batch [340/408] time 0.231 (0.234) data 0.008 (0.012) loss 1.0319 (0.3510) lr 1.2500e-03 eta 0:36:54\n",
            "epoch [27/50] batch [360/408] time 0.238 (0.234) data 0.012 (0.012) loss 0.2835 (0.3508) lr 1.2500e-03 eta 0:36:50\n",
            "epoch [27/50] batch [380/408] time 0.234 (0.234) data 0.016 (0.012) loss 0.2681 (0.3521) lr 1.2500e-03 eta 0:36:45\n",
            "epoch [27/50] batch [400/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.3592 (0.3512) lr 1.2500e-03 eta 0:36:41\n",
            "epoch [28/50] batch [20/408] time 0.234 (0.233) data 0.012 (0.011) loss 0.1988 (0.3240) lr 1.1715e-03 eta 0:36:22\n",
            "epoch [28/50] batch [40/408] time 0.239 (0.235) data 0.019 (0.013) loss 0.2396 (0.2954) lr 1.1715e-03 eta 0:36:38\n",
            "epoch [28/50] batch [60/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1871 (0.2737) lr 1.1715e-03 eta 0:36:29\n",
            "epoch [28/50] batch [80/408] time 0.232 (0.235) data 0.010 (0.012) loss 0.4354 (0.2916) lr 1.1715e-03 eta 0:36:22\n",
            "epoch [28/50] batch [100/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3508 (0.2832) lr 1.1715e-03 eta 0:36:20\n",
            "epoch [28/50] batch [120/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.2019 (0.2779) lr 1.1715e-03 eta 0:36:13\n",
            "epoch [28/50] batch [140/408] time 0.243 (0.235) data 0.020 (0.013) loss 0.2430 (0.2729) lr 1.1715e-03 eta 0:36:12\n",
            "epoch [28/50] batch [160/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2367 (0.2865) lr 1.1715e-03 eta 0:36:06\n",
            "epoch [28/50] batch [180/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2556 (0.2872) lr 1.1715e-03 eta 0:36:01\n",
            "epoch [28/50] batch [200/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3560 (0.2893) lr 1.1715e-03 eta 0:35:57\n",
            "epoch [28/50] batch [220/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.1867 (0.2864) lr 1.1715e-03 eta 0:35:51\n",
            "epoch [28/50] batch [240/408] time 0.244 (0.235) data 0.019 (0.013) loss 0.3412 (0.2981) lr 1.1715e-03 eta 0:35:47\n",
            "epoch [28/50] batch [260/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.4047 (0.2969) lr 1.1715e-03 eta 0:35:42\n",
            "epoch [28/50] batch [280/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.3532 (0.2939) lr 1.1715e-03 eta 0:35:37\n",
            "epoch [28/50] batch [300/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.5117 (0.2907) lr 1.1715e-03 eta 0:35:33\n",
            "epoch [28/50] batch [320/408] time 0.240 (0.235) data 0.012 (0.013) loss 0.3025 (0.2929) lr 1.1715e-03 eta 0:35:29\n",
            "epoch [28/50] batch [340/408] time 0.248 (0.235) data 0.019 (0.013) loss 0.2215 (0.2992) lr 1.1715e-03 eta 0:35:25\n",
            "epoch [28/50] batch [360/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2554 (0.3034) lr 1.1715e-03 eta 0:35:20\n",
            "epoch [28/50] batch [380/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2971 (0.3068) lr 1.1715e-03 eta 0:35:15\n",
            "epoch [28/50] batch [400/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.1897 (0.3085) lr 1.1715e-03 eta 0:35:11\n",
            "epoch [29/50] batch [20/408] time 0.236 (0.235) data 0.014 (0.012) loss 0.2378 (0.3120) lr 1.0933e-03 eta 0:35:01\n",
            "epoch [29/50] batch [40/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.2005 (0.3650) lr 1.0933e-03 eta 0:35:07\n",
            "epoch [29/50] batch [60/408] time 0.240 (0.235) data 0.018 (0.013) loss 0.3479 (0.3626) lr 1.0933e-03 eta 0:34:59\n",
            "epoch [29/50] batch [80/408] time 0.237 (0.235) data 0.018 (0.013) loss 0.1937 (0.3812) lr 1.0933e-03 eta 0:34:54\n",
            "epoch [29/50] batch [100/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2655 (0.3624) lr 1.0933e-03 eta 0:34:49\n",
            "epoch [29/50] batch [120/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.1886 (0.3480) lr 1.0933e-03 eta 0:34:43\n",
            "epoch [29/50] batch [140/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.4817 (0.3511) lr 1.0933e-03 eta 0:34:40\n",
            "epoch [29/50] batch [160/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2122 (0.3599) lr 1.0933e-03 eta 0:34:34\n",
            "epoch [29/50] batch [180/408] time 0.241 (0.235) data 0.018 (0.013) loss 0.2828 (0.3576) lr 1.0933e-03 eta 0:34:30\n",
            "epoch [29/50] batch [200/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.1814 (0.3514) lr 1.0933e-03 eta 0:34:25\n",
            "epoch [29/50] batch [220/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3265 (0.3437) lr 1.0933e-03 eta 0:34:20\n",
            "epoch [29/50] batch [240/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2045 (0.3374) lr 1.0933e-03 eta 0:34:17\n",
            "epoch [29/50] batch [260/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2284 (0.3362) lr 1.0933e-03 eta 0:34:11\n",
            "epoch [29/50] batch [280/408] time 0.241 (0.235) data 0.020 (0.013) loss 0.1774 (0.3300) lr 1.0933e-03 eta 0:34:07\n",
            "epoch [29/50] batch [300/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2184 (0.3252) lr 1.0933e-03 eta 0:34:02\n",
            "epoch [29/50] batch [320/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2069 (0.3284) lr 1.0933e-03 eta 0:33:57\n",
            "epoch [29/50] batch [340/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.1950 (0.3423) lr 1.0933e-03 eta 0:33:53\n",
            "epoch [29/50] batch [360/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2047 (0.3409) lr 1.0933e-03 eta 0:33:48\n",
            "epoch [29/50] batch [380/408] time 0.240 (0.235) data 0.016 (0.013) loss 0.2598 (0.3412) lr 1.0933e-03 eta 0:33:44\n",
            "epoch [29/50] batch [400/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2009 (0.3387) lr 1.0933e-03 eta 0:33:39\n",
            "epoch [30/50] batch [20/408] time 0.248 (0.236) data 0.020 (0.013) loss 0.2333 (0.2801) lr 1.0158e-03 eta 0:33:35\n",
            "epoch [30/50] batch [40/408] time 0.235 (0.236) data 0.012 (0.013) loss 2.1267 (0.4257) lr 1.0158e-03 eta 0:33:34\n",
            "epoch [30/50] batch [60/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.6296 (0.3872) lr 1.0158e-03 eta 0:33:23\n",
            "epoch [30/50] batch [80/408] time 0.234 (0.236) data 0.011 (0.013) loss 0.3004 (0.3565) lr 1.0158e-03 eta 0:33:23\n",
            "epoch [30/50] batch [100/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.1860 (0.3600) lr 1.0158e-03 eta 0:33:17\n",
            "epoch [30/50] batch [120/408] time 0.245 (0.236) data 0.020 (0.013) loss 0.2643 (0.3487) lr 1.0158e-03 eta 0:33:11\n",
            "epoch [30/50] batch [140/408] time 0.233 (0.236) data 0.012 (0.013) loss 0.2406 (0.3448) lr 1.0158e-03 eta 0:33:07\n",
            "epoch [30/50] batch [160/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.2191 (0.3472) lr 1.0158e-03 eta 0:33:00\n",
            "epoch [30/50] batch [180/408] time 0.232 (0.236) data 0.011 (0.013) loss 0.2114 (0.3397) lr 1.0158e-03 eta 0:32:56\n",
            "epoch [30/50] batch [200/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.6941 (0.3451) lr 1.0158e-03 eta 0:32:51\n",
            "epoch [30/50] batch [220/408] time 0.239 (0.236) data 0.018 (0.013) loss 0.6256 (0.3404) lr 1.0158e-03 eta 0:32:46\n",
            "epoch [30/50] batch [240/408] time 0.233 (0.236) data 0.011 (0.013) loss 1.6365 (0.3386) lr 1.0158e-03 eta 0:32:41\n",
            "epoch [30/50] batch [260/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2671 (0.3366) lr 1.0158e-03 eta 0:32:36\n",
            "epoch [30/50] batch [280/408] time 0.233 (0.236) data 0.012 (0.013) loss 0.2417 (0.3366) lr 1.0158e-03 eta 0:32:31\n",
            "epoch [30/50] batch [300/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2928 (0.3510) lr 1.0158e-03 eta 0:32:26\n",
            "epoch [30/50] batch [320/408] time 0.238 (0.235) data 0.015 (0.013) loss 0.2988 (0.3601) lr 1.0158e-03 eta 0:32:21\n",
            "epoch [30/50] batch [340/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2473 (0.3684) lr 1.0158e-03 eta 0:32:17\n",
            "epoch [30/50] batch [360/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.1804 (0.3682) lr 1.0158e-03 eta 0:32:12\n",
            "epoch [30/50] batch [380/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2499 (0.3642) lr 1.0158e-03 eta 0:32:07\n",
            "epoch [30/50] batch [400/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2556 (0.3662) lr 1.0158e-03 eta 0:32:02\n",
            "epoch [31/50] batch [20/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.2937 (0.2929) lr 9.3914e-04 eta 0:32:02\n",
            "epoch [31/50] batch [40/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2491 (0.3437) lr 9.3914e-04 eta 0:31:49\n",
            "epoch [31/50] batch [60/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.1970 (0.4269) lr 9.3914e-04 eta 0:31:43\n",
            "epoch [31/50] batch [80/408] time 0.238 (0.236) data 0.012 (0.013) loss 0.3044 (0.4160) lr 9.3914e-04 eta 0:31:42\n",
            "epoch [31/50] batch [100/408] time 0.238 (0.235) data 0.016 (0.013) loss 1.4645 (0.3957) lr 9.3914e-04 eta 0:31:36\n",
            "epoch [31/50] batch [120/408] time 0.236 (0.236) data 0.014 (0.013) loss 0.1958 (0.3690) lr 9.3914e-04 eta 0:31:34\n",
            "epoch [31/50] batch [140/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2673 (0.3641) lr 9.3914e-04 eta 0:31:28\n",
            "epoch [31/50] batch [160/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.1910 (0.3593) lr 9.3914e-04 eta 0:31:22\n",
            "epoch [31/50] batch [180/408] time 0.237 (0.236) data 0.012 (0.013) loss 0.3661 (0.3579) lr 9.3914e-04 eta 0:31:19\n",
            "epoch [31/50] batch [200/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.2096 (0.3628) lr 9.3914e-04 eta 0:31:14\n",
            "epoch [31/50] batch [220/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.1992 (0.3688) lr 9.3914e-04 eta 0:31:10\n",
            "epoch [31/50] batch [240/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2294 (0.3637) lr 9.3914e-04 eta 0:31:05\n",
            "epoch [31/50] batch [260/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2486 (0.3633) lr 9.3914e-04 eta 0:30:59\n",
            "epoch [31/50] batch [280/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.1937 (0.3581) lr 9.3914e-04 eta 0:30:56\n",
            "epoch [31/50] batch [300/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.2410 (0.3578) lr 9.3914e-04 eta 0:30:51\n",
            "epoch [31/50] batch [320/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.2077 (0.3560) lr 9.3914e-04 eta 0:30:47\n",
            "epoch [31/50] batch [340/408] time 0.231 (0.236) data 0.010 (0.013) loss 0.3463 (0.3563) lr 9.3914e-04 eta 0:30:42\n",
            "epoch [31/50] batch [360/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.3001 (0.3543) lr 9.3914e-04 eta 0:30:36\n",
            "epoch [31/50] batch [380/408] time 0.233 (0.236) data 0.013 (0.013) loss 0.1788 (0.3503) lr 9.3914e-04 eta 0:30:32\n",
            "epoch [31/50] batch [400/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.1887 (0.3525) lr 9.3914e-04 eta 0:30:27\n",
            "epoch [32/50] batch [20/408] time 0.232 (0.238) data 0.013 (0.014) loss 0.1783 (0.2781) lr 8.6373e-04 eta 0:30:36\n",
            "epoch [32/50] batch [40/408] time 0.234 (0.236) data 0.011 (0.013) loss 0.2079 (0.3264) lr 8.6373e-04 eta 0:30:20\n",
            "epoch [32/50] batch [60/408] time 0.233 (0.236) data 0.013 (0.014) loss 0.2882 (0.3238) lr 8.6373e-04 eta 0:30:16\n",
            "epoch [32/50] batch [80/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.2010 (0.3015) lr 8.6373e-04 eta 0:30:09\n",
            "epoch [32/50] batch [100/408] time 0.235 (0.235) data 0.010 (0.013) loss 0.2428 (0.2907) lr 8.6373e-04 eta 0:30:01\n",
            "epoch [32/50] batch [120/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.2327 (0.2866) lr 8.6373e-04 eta 0:29:58\n",
            "epoch [32/50] batch [140/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2310 (0.2919) lr 8.6373e-04 eta 0:29:52\n",
            "epoch [32/50] batch [160/408] time 0.234 (0.236) data 0.012 (0.013) loss 0.3211 (0.2890) lr 8.6373e-04 eta 0:29:48\n",
            "epoch [32/50] batch [180/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2837 (0.2865) lr 8.6373e-04 eta 0:29:43\n",
            "epoch [32/50] batch [200/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2116 (0.2834) lr 8.6373e-04 eta 0:29:37\n",
            "epoch [32/50] batch [220/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2855 (0.2905) lr 8.6373e-04 eta 0:29:33\n",
            "epoch [32/50] batch [240/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.3817 (0.2967) lr 8.6373e-04 eta 0:29:27\n",
            "epoch [32/50] batch [260/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.2079 (0.2957) lr 8.6373e-04 eta 0:29:23\n",
            "epoch [32/50] batch [280/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.2362 (0.3115) lr 8.6373e-04 eta 0:29:17\n",
            "epoch [32/50] batch [300/408] time 0.236 (0.235) data 0.011 (0.013) loss 0.6840 (0.3150) lr 8.6373e-04 eta 0:29:12\n",
            "epoch [32/50] batch [320/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.1784 (0.3146) lr 8.6373e-04 eta 0:29:08\n",
            "epoch [32/50] batch [340/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.1947 (0.3190) lr 8.6373e-04 eta 0:29:03\n",
            "epoch [32/50] batch [360/408] time 0.237 (0.235) data 0.015 (0.013) loss 0.2408 (0.3152) lr 8.6373e-04 eta 0:28:59\n",
            "epoch [32/50] batch [380/408] time 0.238 (0.235) data 0.014 (0.013) loss 0.2596 (0.3217) lr 8.6373e-04 eta 0:28:54\n",
            "epoch [32/50] batch [400/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.3180 (0.3190) lr 8.6373e-04 eta 0:28:49\n",
            "epoch [33/50] batch [20/408] time 0.236 (0.235) data 0.010 (0.013) loss 0.3785 (0.3245) lr 7.8984e-04 eta 0:28:44\n",
            "epoch [33/50] batch [40/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.2178 (0.3308) lr 7.8984e-04 eta 0:28:35\n",
            "epoch [33/50] batch [60/408] time 0.233 (0.235) data 0.012 (0.012) loss 0.3571 (0.3007) lr 7.8984e-04 eta 0:28:33\n",
            "epoch [33/50] batch [80/408] time 0.235 (0.235) data 0.012 (0.012) loss 0.2371 (0.3498) lr 7.8984e-04 eta 0:28:24\n",
            "epoch [33/50] batch [100/408] time 0.241 (0.235) data 0.018 (0.012) loss 0.1777 (0.3655) lr 7.8984e-04 eta 0:28:20\n",
            "epoch [33/50] batch [120/408] time 0.231 (0.235) data 0.011 (0.012) loss 0.2900 (0.3908) lr 7.8984e-04 eta 0:28:14\n",
            "epoch [33/50] batch [140/408] time 0.232 (0.234) data 0.011 (0.012) loss 0.2015 (0.3795) lr 7.8984e-04 eta 0:28:08\n",
            "epoch [33/50] batch [160/408] time 0.235 (0.235) data 0.011 (0.012) loss 0.2418 (0.3697) lr 7.8984e-04 eta 0:28:04\n",
            "epoch [33/50] batch [180/408] time 0.230 (0.234) data 0.009 (0.012) loss 0.2018 (0.3874) lr 7.8984e-04 eta 0:27:58\n",
            "epoch [33/50] batch [200/408] time 0.239 (0.234) data 0.016 (0.012) loss 0.2463 (0.3830) lr 7.8984e-04 eta 0:27:53\n",
            "epoch [33/50] batch [220/408] time 0.234 (0.234) data 0.010 (0.012) loss 0.2429 (0.3841) lr 7.8984e-04 eta 0:27:48\n",
            "epoch [33/50] batch [240/408] time 0.232 (0.234) data 0.012 (0.012) loss 0.2814 (0.3826) lr 7.8984e-04 eta 0:27:43\n",
            "epoch [33/50] batch [260/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.2481 (0.3769) lr 7.8984e-04 eta 0:27:40\n",
            "epoch [33/50] batch [280/408] time 0.231 (0.234) data 0.010 (0.012) loss 0.2572 (0.3684) lr 7.8984e-04 eta 0:27:34\n",
            "epoch [33/50] batch [300/408] time 0.240 (0.234) data 0.018 (0.012) loss 0.2303 (0.3714) lr 7.8984e-04 eta 0:27:30\n",
            "epoch [33/50] batch [320/408] time 0.239 (0.234) data 0.016 (0.012) loss 0.2021 (0.3641) lr 7.8984e-04 eta 0:27:26\n",
            "epoch [33/50] batch [340/408] time 0.233 (0.234) data 0.010 (0.012) loss 0.6038 (0.3640) lr 7.8984e-04 eta 0:27:22\n",
            "epoch [33/50] batch [360/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2168 (0.3645) lr 7.8984e-04 eta 0:27:18\n",
            "epoch [33/50] batch [380/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.5012 (0.3676) lr 7.8984e-04 eta 0:27:13\n",
            "epoch [33/50] batch [400/408] time 0.240 (0.235) data 0.016 (0.013) loss 1.2388 (0.3660) lr 7.8984e-04 eta 0:27:09\n",
            "epoch [34/50] batch [20/408] time 0.234 (0.234) data 0.010 (0.011) loss 0.2891 (0.2642) lr 7.1778e-04 eta 0:26:56\n",
            "epoch [34/50] batch [40/408] time 0.244 (0.234) data 0.021 (0.012) loss 0.1842 (0.2671) lr 7.1778e-04 eta 0:26:54\n",
            "epoch [34/50] batch [60/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.5764 (0.2978) lr 7.1778e-04 eta 0:26:55\n",
            "epoch [34/50] batch [80/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.2103 (0.2833) lr 7.1778e-04 eta 0:26:49\n",
            "epoch [34/50] batch [100/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.9892 (0.2877) lr 7.1778e-04 eta 0:26:46\n",
            "epoch [34/50] batch [120/408] time 0.233 (0.235) data 0.012 (0.013) loss 1.9356 (0.3020) lr 7.1778e-04 eta 0:26:40\n",
            "epoch [34/50] batch [140/408] time 0.244 (0.235) data 0.017 (0.013) loss 0.1910 (0.3163) lr 7.1778e-04 eta 0:26:37\n",
            "epoch [34/50] batch [160/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2174 (0.3095) lr 7.1778e-04 eta 0:26:33\n",
            "epoch [34/50] batch [180/408] time 0.241 (0.235) data 0.018 (0.013) loss 0.4596 (0.3101) lr 7.1778e-04 eta 0:26:29\n",
            "epoch [34/50] batch [200/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2208 (0.3054) lr 7.1778e-04 eta 0:26:25\n",
            "epoch [34/50] batch [220/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.3611 (0.3022) lr 7.1778e-04 eta 0:26:20\n",
            "epoch [34/50] batch [240/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.2208 (0.3107) lr 7.1778e-04 eta 0:26:15\n",
            "epoch [34/50] batch [260/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.1754 (0.3073) lr 7.1778e-04 eta 0:26:11\n",
            "epoch [34/50] batch [280/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.2065 (0.3057) lr 7.1778e-04 eta 0:26:06\n",
            "epoch [34/50] batch [300/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.8438 (0.3079) lr 7.1778e-04 eta 0:26:02\n",
            "epoch [34/50] batch [320/408] time 0.238 (0.235) data 0.012 (0.013) loss 0.1965 (0.3050) lr 7.1778e-04 eta 0:25:57\n",
            "epoch [34/50] batch [340/408] time 0.237 (0.235) data 0.016 (0.013) loss 1.6629 (0.3073) lr 7.1778e-04 eta 0:25:52\n",
            "epoch [34/50] batch [360/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.1902 (0.3077) lr 7.1778e-04 eta 0:25:48\n",
            "epoch [34/50] batch [380/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2825 (0.3043) lr 7.1778e-04 eta 0:25:43\n",
            "epoch [34/50] batch [400/408] time 0.233 (0.236) data 0.010 (0.013) loss 0.2168 (0.3204) lr 7.1778e-04 eta 0:25:39\n",
            "epoch [35/50] batch [20/408] time 0.232 (0.234) data 0.011 (0.012) loss 0.2479 (0.3703) lr 6.4781e-04 eta 0:25:24\n",
            "epoch [35/50] batch [40/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.6789 (0.3702) lr 6.4781e-04 eta 0:25:26\n",
            "epoch [35/50] batch [60/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.3314 (0.3396) lr 6.4781e-04 eta 0:25:20\n",
            "epoch [35/50] batch [80/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.1873 (0.3206) lr 6.4781e-04 eta 0:25:15\n",
            "epoch [35/50] batch [100/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2153 (0.3301) lr 6.4781e-04 eta 0:25:12\n",
            "epoch [35/50] batch [120/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1825 (0.3534) lr 6.4781e-04 eta 0:25:06\n",
            "epoch [35/50] batch [140/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2068 (0.3769) lr 6.4781e-04 eta 0:25:03\n",
            "epoch [35/50] batch [160/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3082 (0.3732) lr 6.4781e-04 eta 0:24:57\n",
            "epoch [35/50] batch [180/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.6764 (0.3762) lr 6.4781e-04 eta 0:24:52\n",
            "epoch [35/50] batch [200/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2482 (0.3645) lr 6.4781e-04 eta 0:24:48\n",
            "epoch [35/50] batch [220/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2371 (0.3550) lr 6.4781e-04 eta 0:24:43\n",
            "epoch [35/50] batch [240/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2305 (0.3650) lr 6.4781e-04 eta 0:24:39\n",
            "epoch [35/50] batch [260/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2218 (0.3630) lr 6.4781e-04 eta 0:24:34\n",
            "epoch [35/50] batch [280/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2303 (0.3549) lr 6.4781e-04 eta 0:24:29\n",
            "epoch [35/50] batch [300/408] time 0.233 (0.235) data 0.012 (0.013) loss 1.3823 (0.3513) lr 6.4781e-04 eta 0:24:25\n",
            "epoch [35/50] batch [320/408] time 0.236 (0.235) data 0.011 (0.013) loss 2.7503 (0.3514) lr 6.4781e-04 eta 0:24:20\n",
            "epoch [35/50] batch [340/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.1926 (0.3543) lr 6.4781e-04 eta 0:24:16\n",
            "epoch [35/50] batch [360/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2419 (0.3654) lr 6.4781e-04 eta 0:24:11\n",
            "epoch [35/50] batch [380/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2249 (0.3584) lr 6.4781e-04 eta 0:24:06\n",
            "epoch [35/50] batch [400/408] time 0.239 (0.235) data 0.018 (0.013) loss 0.2029 (0.3574) lr 6.4781e-04 eta 0:24:01\n",
            "epoch [36/50] batch [20/408] time 0.230 (0.234) data 0.012 (0.012) loss 0.2930 (0.3812) lr 5.8022e-04 eta 0:23:48\n",
            "epoch [36/50] batch [40/408] time 0.232 (0.236) data 0.012 (0.013) loss 0.2251 (0.3078) lr 5.8022e-04 eta 0:23:53\n",
            "epoch [36/50] batch [60/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3434 (0.2997) lr 5.8022e-04 eta 0:23:44\n",
            "epoch [36/50] batch [80/408] time 0.243 (0.236) data 0.018 (0.013) loss 0.2193 (0.2845) lr 5.8022e-04 eta 0:23:44\n",
            "epoch [36/50] batch [100/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2049 (0.2742) lr 5.8022e-04 eta 0:23:37\n",
            "epoch [36/50] batch [120/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2407 (0.3071) lr 5.8022e-04 eta 0:23:31\n",
            "epoch [36/50] batch [140/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1940 (0.2989) lr 5.8022e-04 eta 0:23:27\n",
            "epoch [36/50] batch [160/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2143 (0.2995) lr 5.8022e-04 eta 0:23:21\n",
            "epoch [36/50] batch [180/408] time 0.242 (0.235) data 0.017 (0.013) loss 0.1860 (0.3011) lr 5.8022e-04 eta 0:23:17\n",
            "epoch [36/50] batch [200/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2869 (0.3077) lr 5.8022e-04 eta 0:23:12\n",
            "epoch [36/50] batch [220/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.1715 (0.3019) lr 5.8022e-04 eta 0:23:07\n",
            "epoch [36/50] batch [240/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.6009 (0.2978) lr 5.8022e-04 eta 0:23:04\n",
            "epoch [36/50] batch [260/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1999 (0.2937) lr 5.8022e-04 eta 0:22:59\n",
            "epoch [36/50] batch [280/408] time 0.239 (0.235) data 0.017 (0.013) loss 0.2296 (0.2980) lr 5.8022e-04 eta 0:22:54\n",
            "epoch [36/50] batch [300/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.9346 (0.3102) lr 5.8022e-04 eta 0:22:49\n",
            "epoch [36/50] batch [320/408] time 0.233 (0.235) data 0.014 (0.013) loss 0.1811 (0.3111) lr 5.8022e-04 eta 0:22:44\n",
            "epoch [36/50] batch [340/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2993 (0.3131) lr 5.8022e-04 eta 0:22:40\n",
            "epoch [36/50] batch [360/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2576 (0.3165) lr 5.8022e-04 eta 0:22:35\n",
            "epoch [36/50] batch [380/408] time 0.239 (0.235) data 0.016 (0.013) loss 0.1785 (0.3119) lr 5.8022e-04 eta 0:22:31\n",
            "epoch [36/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1717 (0.3113) lr 5.8022e-04 eta 0:22:26\n",
            "epoch [37/50] batch [20/408] time 0.240 (0.236) data 0.018 (0.014) loss 0.3425 (0.3283) lr 5.1527e-04 eta 0:22:21\n",
            "epoch [37/50] batch [40/408] time 0.235 (0.236) data 0.012 (0.014) loss 0.1753 (0.3282) lr 5.1527e-04 eta 0:22:16\n",
            "epoch [37/50] batch [60/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2783 (0.3675) lr 5.1527e-04 eta 0:22:07\n",
            "epoch [37/50] batch [80/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2193 (0.3593) lr 5.1527e-04 eta 0:22:04\n",
            "epoch [37/50] batch [100/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2522 (0.3790) lr 5.1527e-04 eta 0:21:58\n",
            "epoch [37/50] batch [120/408] time 0.239 (0.235) data 0.017 (0.013) loss 0.2544 (0.3626) lr 5.1527e-04 eta 0:21:53\n",
            "epoch [37/50] batch [140/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2212 (0.3700) lr 5.1527e-04 eta 0:21:49\n",
            "epoch [37/50] batch [160/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1669 (0.3570) lr 5.1527e-04 eta 0:21:43\n",
            "epoch [37/50] batch [180/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.2434 (0.3499) lr 5.1527e-04 eta 0:21:40\n",
            "epoch [37/50] batch [200/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.2104 (0.3455) lr 5.1527e-04 eta 0:21:34\n",
            "epoch [37/50] batch [220/408] time 0.240 (0.235) data 0.018 (0.013) loss 0.2612 (0.3445) lr 5.1527e-04 eta 0:21:30\n",
            "epoch [37/50] batch [240/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.1820 (0.3404) lr 5.1527e-04 eta 0:21:25\n",
            "epoch [37/50] batch [260/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.2214 (0.3434) lr 5.1527e-04 eta 0:21:20\n",
            "epoch [37/50] batch [280/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2209 (0.3384) lr 5.1527e-04 eta 0:21:17\n",
            "epoch [37/50] batch [300/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2794 (0.3356) lr 5.1527e-04 eta 0:21:12\n",
            "epoch [37/50] batch [320/408] time 0.240 (0.235) data 0.019 (0.013) loss 0.2096 (0.3293) lr 5.1527e-04 eta 0:21:07\n",
            "epoch [37/50] batch [340/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.3108 (0.3294) lr 5.1527e-04 eta 0:21:02\n",
            "epoch [37/50] batch [360/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3459 (0.3398) lr 5.1527e-04 eta 0:20:57\n",
            "epoch [37/50] batch [380/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2317 (0.3600) lr 5.1527e-04 eta 0:20:53\n",
            "epoch [37/50] batch [400/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.1724 (0.3597) lr 5.1527e-04 eta 0:20:48\n",
            "epoch [38/50] batch [20/408] time 0.232 (0.237) data 0.010 (0.014) loss 0.3628 (0.3789) lr 4.5322e-04 eta 0:20:50\n",
            "epoch [38/50] batch [40/408] time 0.240 (0.236) data 0.015 (0.013) loss 0.2302 (0.3062) lr 4.5322e-04 eta 0:20:41\n",
            "epoch [38/50] batch [60/408] time 0.239 (0.236) data 0.011 (0.013) loss 0.4121 (0.2858) lr 4.5322e-04 eta 0:20:35\n",
            "epoch [38/50] batch [80/408] time 0.231 (0.236) data 0.011 (0.013) loss 0.2963 (0.3048) lr 4.5322e-04 eta 0:20:32\n",
            "epoch [38/50] batch [100/408] time 0.235 (0.236) data 0.013 (0.013) loss 0.2022 (0.2986) lr 4.5322e-04 eta 0:20:26\n",
            "epoch [38/50] batch [120/408] time 0.232 (0.236) data 0.011 (0.013) loss 0.2606 (0.3332) lr 4.5322e-04 eta 0:20:22\n",
            "epoch [38/50] batch [140/408] time 0.235 (0.236) data 0.012 (0.013) loss 0.1867 (0.3314) lr 4.5322e-04 eta 0:20:16\n",
            "epoch [38/50] batch [160/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.5998 (0.3356) lr 4.5322e-04 eta 0:20:10\n",
            "epoch [38/50] batch [180/408] time 0.234 (0.235) data 0.011 (0.013) loss 2.4285 (0.3405) lr 4.5322e-04 eta 0:20:06\n",
            "epoch [38/50] batch [200/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2022 (0.3412) lr 4.5322e-04 eta 0:20:00\n",
            "epoch [38/50] batch [220/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2920 (0.3509) lr 4.5322e-04 eta 0:19:56\n",
            "epoch [38/50] batch [240/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.3693 (0.3580) lr 4.5322e-04 eta 0:19:51\n",
            "epoch [38/50] batch [260/408] time 0.239 (0.235) data 0.011 (0.013) loss 0.3479 (0.3524) lr 4.5322e-04 eta 0:19:46\n",
            "epoch [38/50] batch [280/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.1717 (0.3471) lr 4.5322e-04 eta 0:19:42\n",
            "epoch [38/50] batch [300/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2569 (0.3436) lr 4.5322e-04 eta 0:19:37\n",
            "epoch [38/50] batch [320/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.1944 (0.3411) lr 4.5322e-04 eta 0:19:33\n",
            "epoch [38/50] batch [340/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1785 (0.3391) lr 4.5322e-04 eta 0:19:27\n",
            "epoch [38/50] batch [360/408] time 0.234 (0.235) data 0.010 (0.013) loss 0.2933 (0.3408) lr 4.5322e-04 eta 0:19:22\n",
            "epoch [38/50] batch [380/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.8561 (0.3377) lr 4.5322e-04 eta 0:19:18\n",
            "epoch [38/50] batch [400/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2333 (0.3368) lr 4.5322e-04 eta 0:19:13\n",
            "epoch [39/50] batch [20/408] time 0.231 (0.238) data 0.013 (0.015) loss 0.1745 (0.3813) lr 3.9432e-04 eta 0:19:21\n",
            "epoch [39/50] batch [40/408] time 0.236 (0.236) data 0.013 (0.014) loss 0.2097 (0.3385) lr 3.9432e-04 eta 0:19:06\n",
            "epoch [39/50] batch [60/408] time 0.236 (0.236) data 0.016 (0.014) loss 0.2843 (0.3414) lr 3.9432e-04 eta 0:19:02\n",
            "epoch [39/50] batch [80/408] time 0.230 (0.236) data 0.010 (0.013) loss 0.5171 (0.3306) lr 3.9432e-04 eta 0:18:54\n",
            "epoch [39/50] batch [100/408] time 0.233 (0.235) data 0.013 (0.013) loss 0.1721 (0.3277) lr 3.9432e-04 eta 0:18:48\n",
            "epoch [39/50] batch [120/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1853 (0.3154) lr 3.9432e-04 eta 0:18:44\n",
            "epoch [39/50] batch [140/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2364 (0.3203) lr 3.9432e-04 eta 0:18:38\n",
            "epoch [39/50] batch [160/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2072 (0.3237) lr 3.9432e-04 eta 0:18:34\n",
            "epoch [39/50] batch [180/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.4944 (0.3165) lr 3.9432e-04 eta 0:18:29\n",
            "epoch [39/50] batch [200/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1886 (0.3091) lr 3.9432e-04 eta 0:18:23\n",
            "epoch [39/50] batch [220/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.1824 (0.3106) lr 3.9432e-04 eta 0:18:20\n",
            "epoch [39/50] batch [240/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.3282 (0.3143) lr 3.9432e-04 eta 0:18:14\n",
            "epoch [39/50] batch [260/408] time 0.241 (0.235) data 0.017 (0.013) loss 0.2374 (0.3101) lr 3.9432e-04 eta 0:18:10\n",
            "epoch [39/50] batch [280/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.2381 (0.3126) lr 3.9432e-04 eta 0:18:05\n",
            "epoch [39/50] batch [300/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2305 (0.3119) lr 3.9432e-04 eta 0:18:00\n",
            "epoch [39/50] batch [320/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1935 (0.3158) lr 3.9432e-04 eta 0:17:56\n",
            "epoch [39/50] batch [340/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.3216 (0.3118) lr 3.9432e-04 eta 0:17:51\n",
            "epoch [39/50] batch [360/408] time 0.238 (0.235) data 0.016 (0.013) loss 0.1891 (0.3129) lr 3.9432e-04 eta 0:17:46\n",
            "epoch [39/50] batch [380/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.3089 (0.3101) lr 3.9432e-04 eta 0:17:41\n",
            "epoch [39/50] batch [400/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.4042 (0.3075) lr 3.9432e-04 eta 0:17:36\n",
            "epoch [40/50] batch [20/408] time 0.234 (0.236) data 0.011 (0.013) loss 0.1985 (0.4778) lr 3.3879e-04 eta 0:17:33\n",
            "epoch [40/50] batch [40/408] time 0.239 (0.235) data 0.014 (0.012) loss 0.3312 (0.4425) lr 3.3879e-04 eta 0:17:23\n",
            "epoch [40/50] batch [60/408] time 0.233 (0.236) data 0.011 (0.013) loss 0.5374 (0.4161) lr 3.3879e-04 eta 0:17:23\n",
            "epoch [40/50] batch [80/408] time 0.232 (0.235) data 0.011 (0.012) loss 0.3177 (0.3807) lr 3.3879e-04 eta 0:17:16\n",
            "epoch [40/50] batch [100/408] time 0.237 (0.235) data 0.017 (0.013) loss 0.2535 (0.3640) lr 3.3879e-04 eta 0:17:11\n",
            "epoch [40/50] batch [120/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2683 (0.3683) lr 3.3879e-04 eta 0:17:06\n",
            "epoch [40/50] batch [140/408] time 0.234 (0.235) data 0.013 (0.012) loss 0.2231 (0.3723) lr 3.3879e-04 eta 0:17:00\n",
            "epoch [40/50] batch [160/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2940 (0.3863) lr 3.3879e-04 eta 0:16:57\n",
            "epoch [40/50] batch [180/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.2600 (0.3752) lr 3.3879e-04 eta 0:16:51\n",
            "epoch [40/50] batch [200/408] time 0.242 (0.235) data 0.021 (0.013) loss 0.1743 (0.3750) lr 3.3879e-04 eta 0:16:47\n",
            "epoch [40/50] batch [220/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1838 (0.3755) lr 3.3879e-04 eta 0:16:42\n",
            "epoch [40/50] batch [240/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2364 (0.3641) lr 3.3879e-04 eta 0:16:37\n",
            "epoch [40/50] batch [260/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.2167 (0.3531) lr 3.3879e-04 eta 0:16:33\n",
            "epoch [40/50] batch [280/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.9169 (0.3482) lr 3.3879e-04 eta 0:16:28\n",
            "epoch [40/50] batch [300/408] time 0.244 (0.235) data 0.020 (0.013) loss 0.1781 (0.3472) lr 3.3879e-04 eta 0:16:24\n",
            "epoch [40/50] batch [320/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2425 (0.3474) lr 3.3879e-04 eta 0:16:19\n",
            "epoch [40/50] batch [340/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.2331 (0.3444) lr 3.3879e-04 eta 0:16:14\n",
            "epoch [40/50] batch [360/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.3738 (0.3380) lr 3.3879e-04 eta 0:16:10\n",
            "epoch [40/50] batch [380/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2497 (0.3355) lr 3.3879e-04 eta 0:16:04\n",
            "epoch [40/50] batch [400/408] time 0.244 (0.235) data 0.021 (0.013) loss 0.2461 (0.3370) lr 3.3879e-04 eta 0:16:00\n",
            "epoch [41/50] batch [20/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.2268 (0.4350) lr 2.8686e-04 eta 0:15:49\n",
            "epoch [41/50] batch [40/408] time 0.238 (0.234) data 0.018 (0.012) loss 0.2052 (0.3431) lr 2.8686e-04 eta 0:15:45\n",
            "epoch [41/50] batch [60/408] time 0.236 (0.234) data 0.016 (0.013) loss 0.2172 (0.3085) lr 2.8686e-04 eta 0:15:42\n",
            "epoch [41/50] batch [80/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.1837 (0.3836) lr 2.8686e-04 eta 0:15:36\n",
            "epoch [41/50] batch [100/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2734 (0.3712) lr 2.8686e-04 eta 0:15:33\n",
            "epoch [41/50] batch [120/408] time 0.233 (0.234) data 0.013 (0.013) loss 0.1969 (0.3576) lr 2.8686e-04 eta 0:15:28\n",
            "epoch [41/50] batch [140/408] time 0.242 (0.235) data 0.022 (0.013) loss 0.1657 (0.3447) lr 2.8686e-04 eta 0:15:24\n",
            "epoch [41/50] batch [160/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2441 (0.3469) lr 2.8686e-04 eta 0:15:19\n",
            "epoch [41/50] batch [180/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.1850 (0.3602) lr 2.8686e-04 eta 0:15:14\n",
            "epoch [41/50] batch [200/408] time 0.230 (0.235) data 0.010 (0.013) loss 0.2329 (0.3598) lr 2.8686e-04 eta 0:15:11\n",
            "epoch [41/50] batch [220/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.2574 (0.3642) lr 2.8686e-04 eta 0:15:06\n",
            "epoch [41/50] batch [240/408] time 0.243 (0.235) data 0.019 (0.013) loss 0.2968 (0.3559) lr 2.8686e-04 eta 0:15:01\n",
            "epoch [41/50] batch [260/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1804 (0.3599) lr 2.8686e-04 eta 0:14:56\n",
            "epoch [41/50] batch [280/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2543 (0.3544) lr 2.8686e-04 eta 0:14:51\n",
            "epoch [41/50] batch [300/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1737 (0.3489) lr 2.8686e-04 eta 0:14:47\n",
            "epoch [41/50] batch [320/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2234 (0.3454) lr 2.8686e-04 eta 0:14:42\n",
            "epoch [41/50] batch [340/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.2001 (0.3465) lr 2.8686e-04 eta 0:14:37\n",
            "epoch [41/50] batch [360/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1930 (0.3533) lr 2.8686e-04 eta 0:14:33\n",
            "epoch [41/50] batch [380/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.6172 (0.3534) lr 2.8686e-04 eta 0:14:28\n",
            "epoch [41/50] batch [400/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2286 (0.3479) lr 2.8686e-04 eta 0:14:23\n",
            "epoch [42/50] batch [20/408] time 0.235 (0.235) data 0.013 (0.012) loss 0.1945 (0.2920) lr 2.3873e-04 eta 0:14:16\n",
            "epoch [42/50] batch [40/408] time 0.233 (0.236) data 0.012 (0.014) loss 0.1836 (0.2691) lr 2.3873e-04 eta 0:14:16\n",
            "epoch [42/50] batch [60/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.2412 (0.2941) lr 2.3873e-04 eta 0:14:09\n",
            "epoch [42/50] batch [80/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.7608 (0.3519) lr 2.3873e-04 eta 0:14:03\n",
            "epoch [42/50] batch [100/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.3053 (0.3507) lr 2.3873e-04 eta 0:14:00\n",
            "epoch [42/50] batch [120/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1801 (0.3441) lr 2.3873e-04 eta 0:13:54\n",
            "epoch [42/50] batch [140/408] time 0.240 (0.235) data 0.017 (0.013) loss 0.2424 (0.3442) lr 2.3873e-04 eta 0:13:50\n",
            "epoch [42/50] batch [160/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.3872 (0.3492) lr 2.3873e-04 eta 0:13:44\n",
            "epoch [42/50] batch [180/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2359 (0.3505) lr 2.3873e-04 eta 0:13:39\n",
            "epoch [42/50] batch [200/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2326 (0.3585) lr 2.3873e-04 eta 0:13:35\n",
            "epoch [42/50] batch [220/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.1730 (0.3524) lr 2.3873e-04 eta 0:13:30\n",
            "epoch [42/50] batch [240/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.1732 (0.3506) lr 2.3873e-04 eta 0:13:25\n",
            "epoch [42/50] batch [260/408] time 0.229 (0.235) data 0.010 (0.013) loss 0.1887 (0.3457) lr 2.3873e-04 eta 0:13:20\n",
            "epoch [42/50] batch [280/408] time 0.234 (0.235) data 0.010 (0.013) loss 3.9573 (0.3673) lr 2.3873e-04 eta 0:13:15\n",
            "epoch [42/50] batch [300/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2397 (0.3590) lr 2.3873e-04 eta 0:13:11\n",
            "epoch [42/50] batch [320/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3175 (0.3618) lr 2.3873e-04 eta 0:13:06\n",
            "epoch [42/50] batch [340/408] time 0.240 (0.235) data 0.018 (0.013) loss 0.2615 (0.3573) lr 2.3873e-04 eta 0:13:02\n",
            "epoch [42/50] batch [360/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2402 (0.3505) lr 2.3873e-04 eta 0:12:57\n",
            "epoch [42/50] batch [380/408] time 0.234 (0.235) data 0.009 (0.013) loss 0.2581 (0.3456) lr 2.3873e-04 eta 0:12:52\n",
            "epoch [42/50] batch [400/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.1760 (0.3493) lr 2.3873e-04 eta 0:12:47\n",
            "epoch [43/50] batch [20/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.3405 (0.3797) lr 1.9459e-04 eta 0:12:41\n",
            "epoch [43/50] batch [40/408] time 0.235 (0.236) data 0.014 (0.013) loss 0.2042 (0.3011) lr 1.9459e-04 eta 0:12:40\n",
            "epoch [43/50] batch [60/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2529 (0.3017) lr 1.9459e-04 eta 0:12:33\n",
            "epoch [43/50] batch [80/408] time 0.239 (0.235) data 0.015 (0.013) loss 0.2321 (0.3509) lr 1.9459e-04 eta 0:12:28\n",
            "epoch [43/50] batch [100/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.1868 (0.3311) lr 1.9459e-04 eta 0:12:23\n",
            "epoch [43/50] batch [120/408] time 0.233 (0.235) data 0.014 (0.013) loss 0.1894 (0.3508) lr 1.9459e-04 eta 0:12:18\n",
            "epoch [43/50] batch [140/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2748 (0.3472) lr 1.9459e-04 eta 0:12:14\n",
            "epoch [43/50] batch [160/408] time 0.231 (0.235) data 0.010 (0.013) loss 0.2624 (0.3515) lr 1.9459e-04 eta 0:12:09\n",
            "epoch [43/50] batch [180/408] time 0.243 (0.235) data 0.021 (0.013) loss 0.1997 (0.3394) lr 1.9459e-04 eta 0:12:04\n",
            "epoch [43/50] batch [200/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.1684 (0.3287) lr 1.9459e-04 eta 0:11:59\n",
            "epoch [43/50] batch [220/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2333 (0.3269) lr 1.9459e-04 eta 0:11:54\n",
            "epoch [43/50] batch [240/408] time 0.231 (0.235) data 0.012 (0.013) loss 0.1845 (0.3183) lr 1.9459e-04 eta 0:11:50\n",
            "epoch [43/50] batch [260/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2365 (0.3188) lr 1.9459e-04 eta 0:11:45\n",
            "epoch [43/50] batch [280/408] time 0.242 (0.235) data 0.020 (0.013) loss 0.1780 (0.3234) lr 1.9459e-04 eta 0:11:40\n",
            "epoch [43/50] batch [300/408] time 0.232 (0.235) data 0.012 (0.013) loss 0.2516 (0.3240) lr 1.9459e-04 eta 0:11:36\n",
            "epoch [43/50] batch [320/408] time 0.235 (0.235) data 0.014 (0.013) loss 0.1805 (0.3219) lr 1.9459e-04 eta 0:11:31\n",
            "epoch [43/50] batch [340/408] time 0.232 (0.235) data 0.013 (0.013) loss 0.1767 (0.3237) lr 1.9459e-04 eta 0:11:26\n",
            "epoch [43/50] batch [360/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2465 (0.3204) lr 1.9459e-04 eta 0:11:21\n",
            "epoch [43/50] batch [380/408] time 0.238 (0.235) data 0.017 (0.013) loss 0.2694 (0.3176) lr 1.9459e-04 eta 0:11:17\n",
            "epoch [43/50] batch [400/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2086 (0.3146) lr 1.9459e-04 eta 0:11:12\n",
            "epoch [44/50] batch [20/408] time 0.242 (0.235) data 0.017 (0.012) loss 0.1684 (0.2633) lr 1.5462e-04 eta 0:11:05\n",
            "epoch [44/50] batch [40/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2165 (0.2796) lr 1.5462e-04 eta 0:11:02\n",
            "epoch [44/50] batch [60/408] time 0.233 (0.235) data 0.012 (0.012) loss 0.2081 (0.2714) lr 1.5462e-04 eta 0:10:56\n",
            "epoch [44/50] batch [80/408] time 0.233 (0.235) data 0.010 (0.013) loss 0.3354 (0.2713) lr 1.5462e-04 eta 0:10:52\n",
            "epoch [44/50] batch [100/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.1874 (0.2654) lr 1.5462e-04 eta 0:10:47\n",
            "epoch [44/50] batch [120/408] time 0.241 (0.235) data 0.016 (0.013) loss 0.2375 (0.2664) lr 1.5462e-04 eta 0:10:42\n",
            "epoch [44/50] batch [140/408] time 0.233 (0.235) data 0.014 (0.013) loss 0.2224 (0.2760) lr 1.5462e-04 eta 0:10:37\n",
            "epoch [44/50] batch [160/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.4006 (0.2702) lr 1.5462e-04 eta 0:10:32\n",
            "epoch [44/50] batch [180/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.7345 (0.2679) lr 1.5462e-04 eta 0:10:28\n",
            "epoch [44/50] batch [200/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.1883 (0.2658) lr 1.5462e-04 eta 0:10:23\n",
            "epoch [44/50] batch [220/408] time 0.246 (0.235) data 0.027 (0.013) loss 0.1974 (0.2656) lr 1.5462e-04 eta 0:10:18\n",
            "epoch [44/50] batch [240/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2126 (0.2677) lr 1.5462e-04 eta 0:10:14\n",
            "epoch [44/50] batch [260/408] time 0.236 (0.235) data 0.013 (0.013) loss 0.2060 (0.2840) lr 1.5462e-04 eta 0:10:09\n",
            "epoch [44/50] batch [280/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.7484 (0.2842) lr 1.5462e-04 eta 0:10:05\n",
            "epoch [44/50] batch [300/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2394 (0.2853) lr 1.5462e-04 eta 0:10:00\n",
            "epoch [44/50] batch [320/408] time 0.244 (0.235) data 0.017 (0.013) loss 0.1969 (0.2913) lr 1.5462e-04 eta 0:09:55\n",
            "epoch [44/50] batch [340/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2283 (0.2912) lr 1.5462e-04 eta 0:09:51\n",
            "epoch [44/50] batch [360/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2487 (0.2960) lr 1.5462e-04 eta 0:09:46\n",
            "epoch [44/50] batch [380/408] time 0.232 (0.235) data 0.009 (0.013) loss 1.3262 (0.3069) lr 1.5462e-04 eta 0:09:41\n",
            "epoch [44/50] batch [400/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.1914 (0.3034) lr 1.5462e-04 eta 0:09:37\n",
            "epoch [45/50] batch [20/408] time 0.234 (0.237) data 0.012 (0.014) loss 0.5601 (0.2389) lr 1.1897e-04 eta 0:09:34\n",
            "epoch [45/50] batch [40/408] time 0.234 (0.235) data 0.013 (0.013) loss 0.1952 (0.2364) lr 1.1897e-04 eta 0:09:26\n",
            "epoch [45/50] batch [60/408] time 0.235 (0.235) data 0.011 (0.012) loss 0.1743 (0.3132) lr 1.1897e-04 eta 0:09:20\n",
            "epoch [45/50] batch [80/408] time 0.236 (0.236) data 0.012 (0.013) loss 0.2173 (0.2983) lr 1.1897e-04 eta 0:09:17\n",
            "epoch [45/50] batch [100/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2280 (0.3106) lr 1.1897e-04 eta 0:09:12\n",
            "epoch [45/50] batch [120/408] time 0.238 (0.236) data 0.012 (0.013) loss 0.2269 (0.3097) lr 1.1897e-04 eta 0:09:08\n",
            "epoch [45/50] batch [140/408] time 0.233 (0.235) data 0.009 (0.013) loss 0.2359 (0.3127) lr 1.1897e-04 eta 0:09:03\n",
            "epoch [45/50] batch [160/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2494 (0.3091) lr 1.1897e-04 eta 0:08:57\n",
            "epoch [45/50] batch [180/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2252 (0.2994) lr 1.1897e-04 eta 0:08:53\n",
            "epoch [45/50] batch [200/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.1644 (0.3125) lr 1.1897e-04 eta 0:08:48\n",
            "epoch [45/50] batch [220/408] time 0.236 (0.235) data 0.015 (0.013) loss 0.2613 (0.3328) lr 1.1897e-04 eta 0:08:44\n",
            "epoch [45/50] batch [240/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.2720 (0.3263) lr 1.1897e-04 eta 0:08:39\n",
            "epoch [45/50] batch [260/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.1709 (0.3188) lr 1.1897e-04 eta 0:08:34\n",
            "epoch [45/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2016 (0.3310) lr 1.1897e-04 eta 0:08:29\n",
            "epoch [45/50] batch [300/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2403 (0.3286) lr 1.1897e-04 eta 0:08:24\n",
            "epoch [45/50] batch [320/408] time 0.237 (0.235) data 0.014 (0.013) loss 0.2532 (0.3272) lr 1.1897e-04 eta 0:08:20\n",
            "epoch [45/50] batch [340/408] time 0.232 (0.235) data 0.010 (0.013) loss 0.2318 (0.3251) lr 1.1897e-04 eta 0:08:15\n",
            "epoch [45/50] batch [360/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.1873 (0.3247) lr 1.1897e-04 eta 0:08:10\n",
            "epoch [45/50] batch [380/408] time 0.241 (0.235) data 0.021 (0.013) loss 0.1805 (0.3279) lr 1.1897e-04 eta 0:08:05\n",
            "epoch [45/50] batch [400/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2832 (0.3305) lr 1.1897e-04 eta 0:08:00\n",
            "epoch [46/50] batch [20/408] time 0.232 (0.236) data 0.010 (0.014) loss 0.1782 (0.2998) lr 8.7779e-05 eta 0:07:57\n",
            "epoch [46/50] batch [40/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2412 (0.2639) lr 8.7779e-05 eta 0:07:49\n",
            "epoch [46/50] batch [60/408] time 0.244 (0.235) data 0.019 (0.013) loss 0.1669 (0.2528) lr 8.7779e-05 eta 0:07:45\n",
            "epoch [46/50] batch [80/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.1927 (0.2687) lr 8.7779e-05 eta 0:07:40\n",
            "epoch [46/50] batch [100/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.1725 (0.2619) lr 8.7779e-05 eta 0:07:34\n",
            "epoch [46/50] batch [120/408] time 0.231 (0.235) data 0.009 (0.013) loss 0.2578 (0.2849) lr 8.7779e-05 eta 0:07:30\n",
            "epoch [46/50] batch [140/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2833 (0.3009) lr 8.7779e-05 eta 0:07:25\n",
            "epoch [46/50] batch [160/408] time 0.240 (0.235) data 0.015 (0.013) loss 0.4572 (0.3049) lr 8.7779e-05 eta 0:07:21\n",
            "epoch [46/50] batch [180/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2028 (0.3238) lr 8.7779e-05 eta 0:07:16\n",
            "epoch [46/50] batch [200/408] time 0.233 (0.235) data 0.012 (0.013) loss 0.2278 (0.3220) lr 8.7779e-05 eta 0:07:11\n",
            "epoch [46/50] batch [220/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.5376 (0.3340) lr 8.7779e-05 eta 0:07:07\n",
            "epoch [46/50] batch [240/408] time 0.230 (0.235) data 0.010 (0.012) loss 0.1930 (0.3360) lr 8.7779e-05 eta 0:07:02\n",
            "epoch [46/50] batch [260/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.2207 (0.3319) lr 8.7779e-05 eta 0:06:57\n",
            "epoch [46/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2481 (0.3376) lr 8.7779e-05 eta 0:06:52\n",
            "epoch [46/50] batch [300/408] time 0.232 (0.235) data 0.011 (0.012) loss 0.2151 (0.3410) lr 8.7779e-05 eta 0:06:48\n",
            "epoch [46/50] batch [320/408] time 0.233 (0.235) data 0.011 (0.013) loss 0.2256 (0.3428) lr 8.7779e-05 eta 0:06:43\n",
            "epoch [46/50] batch [340/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2186 (0.3524) lr 8.7779e-05 eta 0:06:38\n",
            "epoch [46/50] batch [360/408] time 0.238 (0.235) data 0.018 (0.013) loss 0.2941 (0.3493) lr 8.7779e-05 eta 0:06:34\n",
            "epoch [46/50] batch [380/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.2243 (0.3490) lr 8.7779e-05 eta 0:06:29\n",
            "epoch [46/50] batch [400/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.3123 (0.3491) lr 8.7779e-05 eta 0:06:24\n",
            "epoch [47/50] batch [20/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2059 (0.2612) lr 6.1179e-05 eta 0:06:18\n",
            "epoch [47/50] batch [40/408] time 0.233 (0.234) data 0.013 (0.012) loss 0.1957 (0.2555) lr 6.1179e-05 eta 0:06:12\n",
            "epoch [47/50] batch [60/408] time 0.232 (0.235) data 0.011 (0.013) loss 0.4694 (0.2691) lr 6.1179e-05 eta 0:06:09\n",
            "epoch [47/50] batch [80/408] time 0.234 (0.234) data 0.013 (0.012) loss 0.1786 (0.2746) lr 6.1179e-05 eta 0:06:03\n",
            "epoch [47/50] batch [100/408] time 0.237 (0.235) data 0.017 (0.013) loss 0.2356 (0.2676) lr 6.1179e-05 eta 0:05:59\n",
            "epoch [47/50] batch [120/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2763 (0.2772) lr 6.1179e-05 eta 0:05:54\n",
            "epoch [47/50] batch [140/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.3245 (0.2780) lr 6.1179e-05 eta 0:05:49\n",
            "epoch [47/50] batch [160/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2524 (0.2777) lr 6.1179e-05 eta 0:05:45\n",
            "epoch [47/50] batch [180/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.1997 (0.2907) lr 6.1179e-05 eta 0:05:40\n",
            "epoch [47/50] batch [200/408] time 0.239 (0.234) data 0.018 (0.012) loss 0.3638 (0.2980) lr 6.1179e-05 eta 0:05:35\n",
            "epoch [47/50] batch [220/408] time 0.236 (0.235) data 0.012 (0.012) loss 0.3114 (0.2952) lr 6.1179e-05 eta 0:05:31\n",
            "epoch [47/50] batch [240/408] time 0.238 (0.234) data 0.014 (0.012) loss 0.2130 (0.3021) lr 6.1179e-05 eta 0:05:26\n",
            "epoch [47/50] batch [260/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.1937 (0.2969) lr 6.1179e-05 eta 0:05:21\n",
            "epoch [47/50] batch [280/408] time 0.232 (0.234) data 0.011 (0.013) loss 0.1799 (0.2914) lr 6.1179e-05 eta 0:05:17\n",
            "epoch [47/50] batch [300/408] time 0.239 (0.234) data 0.016 (0.012) loss 0.2317 (0.2925) lr 6.1179e-05 eta 0:05:12\n",
            "epoch [47/50] batch [320/408] time 0.234 (0.234) data 0.013 (0.013) loss 0.1766 (0.2914) lr 6.1179e-05 eta 0:05:07\n",
            "epoch [47/50] batch [340/408] time 0.232 (0.234) data 0.012 (0.013) loss 0.2669 (0.2920) lr 6.1179e-05 eta 0:05:02\n",
            "epoch [47/50] batch [360/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.4572 (0.2956) lr 6.1179e-05 eta 0:04:58\n",
            "epoch [47/50] batch [380/408] time 0.235 (0.234) data 0.012 (0.013) loss 0.1843 (0.2962) lr 6.1179e-05 eta 0:04:53\n",
            "epoch [47/50] batch [400/408] time 0.235 (0.234) data 0.011 (0.013) loss 0.2616 (0.3020) lr 6.1179e-05 eta 0:04:48\n",
            "epoch [48/50] batch [20/408] time 0.234 (0.234) data 0.012 (0.012) loss 0.2621 (0.5697) lr 3.9271e-05 eta 0:04:42\n",
            "epoch [48/50] batch [40/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2893 (0.4159) lr 3.9271e-05 eta 0:04:37\n",
            "epoch [48/50] batch [60/408] time 0.236 (0.235) data 0.012 (0.013) loss 0.2544 (0.3747) lr 3.9271e-05 eta 0:04:33\n",
            "epoch [48/50] batch [80/408] time 0.232 (0.235) data 0.010 (0.012) loss 0.2296 (0.3561) lr 3.9271e-05 eta 0:04:28\n",
            "epoch [48/50] batch [100/408] time 0.236 (0.235) data 0.014 (0.013) loss 0.2471 (0.3549) lr 3.9271e-05 eta 0:04:24\n",
            "epoch [48/50] batch [120/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.3218 (0.3471) lr 3.9271e-05 eta 0:04:19\n",
            "epoch [48/50] batch [140/408] time 0.238 (0.235) data 0.011 (0.013) loss 0.9932 (0.3889) lr 3.9271e-05 eta 0:04:14\n",
            "epoch [48/50] batch [160/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2358 (0.3788) lr 3.9271e-05 eta 0:04:10\n",
            "epoch [48/50] batch [180/408] time 0.236 (0.235) data 0.011 (0.013) loss 0.2477 (0.3705) lr 3.9271e-05 eta 0:04:05\n",
            "epoch [48/50] batch [200/408] time 0.234 (0.235) data 0.011 (0.013) loss 0.2588 (0.3643) lr 3.9271e-05 eta 0:04:00\n",
            "epoch [48/50] batch [220/408] time 0.230 (0.235) data 0.011 (0.013) loss 0.1794 (0.3592) lr 3.9271e-05 eta 0:03:56\n",
            "epoch [48/50] batch [240/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.2839 (0.3473) lr 3.9271e-05 eta 0:03:51\n",
            "epoch [48/50] batch [260/408] time 0.244 (0.235) data 0.022 (0.013) loss 0.1869 (0.3460) lr 3.9271e-05 eta 0:03:46\n",
            "epoch [48/50] batch [280/408] time 0.234 (0.235) data 0.012 (0.013) loss 0.2168 (0.3464) lr 3.9271e-05 eta 0:03:41\n",
            "epoch [48/50] batch [300/408] time 0.235 (0.235) data 0.011 (0.013) loss 0.3015 (0.3498) lr 3.9271e-05 eta 0:03:37\n",
            "epoch [48/50] batch [320/408] time 0.233 (0.235) data 0.011 (0.013) loss 1.6272 (0.3541) lr 3.9271e-05 eta 0:03:32\n",
            "epoch [48/50] batch [340/408] time 0.237 (0.235) data 0.013 (0.013) loss 0.2203 (0.3520) lr 3.9271e-05 eta 0:03:27\n",
            "epoch [48/50] batch [360/408] time 0.231 (0.235) data 0.011 (0.013) loss 0.3743 (0.3541) lr 3.9271e-05 eta 0:03:23\n",
            "epoch [48/50] batch [380/408] time 0.235 (0.235) data 0.013 (0.013) loss 0.1664 (0.3605) lr 3.9271e-05 eta 0:03:18\n",
            "epoch [48/50] batch [400/408] time 0.237 (0.235) data 0.012 (0.013) loss 0.2283 (0.3566) lr 3.9271e-05 eta 0:03:13\n",
            "epoch [49/50] batch [20/408] time 0.232 (0.234) data 0.011 (0.011) loss 0.1824 (0.2706) lr 2.2141e-05 eta 0:03:06\n",
            "epoch [49/50] batch [40/408] time 0.239 (0.235) data 0.014 (0.012) loss 0.1976 (0.2479) lr 2.2141e-05 eta 0:03:02\n",
            "epoch [49/50] batch [60/408] time 0.234 (0.235) data 0.012 (0.012) loss 0.1987 (0.2496) lr 2.2141e-05 eta 0:02:57\n",
            "epoch [49/50] batch [80/408] time 0.233 (0.234) data 0.011 (0.012) loss 0.2475 (0.3095) lr 2.2141e-05 eta 0:02:52\n",
            "epoch [49/50] batch [100/408] time 0.229 (0.235) data 0.009 (0.012) loss 0.3121 (0.3033) lr 2.2141e-05 eta 0:02:47\n",
            "epoch [49/50] batch [120/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2081 (0.3029) lr 2.2141e-05 eta 0:02:43\n",
            "epoch [49/50] batch [140/408] time 0.239 (0.234) data 0.019 (0.012) loss 0.4113 (0.3164) lr 2.2141e-05 eta 0:02:38\n",
            "epoch [49/50] batch [160/408] time 0.233 (0.234) data 0.010 (0.012) loss 0.2625 (0.3115) lr 2.2141e-05 eta 0:02:33\n",
            "epoch [49/50] batch [180/408] time 0.234 (0.234) data 0.011 (0.012) loss 0.2124 (0.3058) lr 2.2141e-05 eta 0:02:29\n",
            "epoch [49/50] batch [200/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2078 (0.3047) lr 2.2141e-05 eta 0:02:24\n",
            "epoch [49/50] batch [220/408] time 0.231 (0.234) data 0.010 (0.012) loss 0.2611 (0.3073) lr 2.2141e-05 eta 0:02:19\n",
            "epoch [49/50] batch [240/408] time 0.241 (0.235) data 0.019 (0.013) loss 0.2299 (0.3133) lr 2.2141e-05 eta 0:02:15\n",
            "epoch [49/50] batch [260/408] time 0.232 (0.234) data 0.012 (0.012) loss 0.2666 (0.3278) lr 2.2141e-05 eta 0:02:10\n",
            "epoch [49/50] batch [280/408] time 0.235 (0.234) data 0.012 (0.012) loss 0.2345 (0.3295) lr 2.2141e-05 eta 0:02:05\n",
            "epoch [49/50] batch [300/408] time 0.237 (0.235) data 0.015 (0.012) loss 0.1814 (0.3427) lr 2.2141e-05 eta 0:02:01\n",
            "epoch [49/50] batch [320/408] time 0.233 (0.235) data 0.011 (0.012) loss 0.3702 (0.3470) lr 2.2141e-05 eta 0:01:56\n",
            "epoch [49/50] batch [340/408] time 0.240 (0.235) data 0.018 (0.012) loss 0.2320 (0.3429) lr 2.2141e-05 eta 0:01:51\n",
            "epoch [49/50] batch [360/408] time 0.234 (0.235) data 0.011 (0.012) loss 0.2237 (0.3384) lr 2.2141e-05 eta 0:01:46\n",
            "epoch [49/50] batch [380/408] time 0.233 (0.235) data 0.011 (0.012) loss 0.1741 (0.3332) lr 2.2141e-05 eta 0:01:42\n",
            "epoch [49/50] batch [400/408] time 0.236 (0.235) data 0.011 (0.012) loss 0.2099 (0.3343) lr 2.2141e-05 eta 0:01:37\n",
            "epoch [50/50] batch [20/408] time 0.233 (0.233) data 0.011 (0.011) loss 0.2565 (0.3472) lr 9.8566e-06 eta 0:01:30\n",
            "epoch [50/50] batch [40/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.1705 (0.2977) lr 9.8566e-06 eta 0:01:26\n",
            "epoch [50/50] batch [60/408] time 0.237 (0.234) data 0.017 (0.012) loss 0.2062 (0.3043) lr 9.8566e-06 eta 0:01:21\n",
            "epoch [50/50] batch [80/408] time 0.239 (0.235) data 0.018 (0.012) loss 0.2983 (0.3258) lr 9.8566e-06 eta 0:01:16\n",
            "epoch [50/50] batch [100/408] time 0.235 (0.235) data 0.010 (0.012) loss 0.2726 (0.3161) lr 9.8566e-06 eta 0:01:12\n",
            "epoch [50/50] batch [120/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.1703 (0.3162) lr 9.8566e-06 eta 0:01:07\n",
            "epoch [50/50] batch [140/408] time 0.235 (0.235) data 0.012 (0.013) loss 0.2457 (0.3362) lr 9.8566e-06 eta 0:01:02\n",
            "epoch [50/50] batch [160/408] time 0.232 (0.234) data 0.011 (0.012) loss 0.1799 (0.3301) lr 9.8566e-06 eta 0:00:58\n",
            "epoch [50/50] batch [180/408] time 0.238 (0.234) data 0.015 (0.012) loss 0.3292 (0.3201) lr 9.8566e-06 eta 0:00:53\n",
            "epoch [50/50] batch [200/408] time 0.229 (0.234) data 0.010 (0.012) loss 0.2567 (0.3165) lr 9.8566e-06 eta 0:00:48\n",
            "epoch [50/50] batch [220/408] time 0.234 (0.234) data 0.011 (0.012) loss 0.3249 (0.3143) lr 9.8566e-06 eta 0:00:44\n",
            "epoch [50/50] batch [240/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.1695 (0.3132) lr 9.8566e-06 eta 0:00:39\n",
            "epoch [50/50] batch [260/408] time 0.236 (0.234) data 0.014 (0.012) loss 0.1708 (0.3119) lr 9.8566e-06 eta 0:00:34\n",
            "epoch [50/50] batch [280/408] time 0.242 (0.234) data 0.020 (0.012) loss 0.6214 (0.3064) lr 9.8566e-06 eta 0:00:30\n",
            "epoch [50/50] batch [300/408] time 0.233 (0.234) data 0.012 (0.012) loss 0.2915 (0.3119) lr 9.8566e-06 eta 0:00:25\n",
            "epoch [50/50] batch [320/408] time 0.235 (0.234) data 0.015 (0.012) loss 0.3552 (0.3070) lr 9.8566e-06 eta 0:00:20\n",
            "epoch [50/50] batch [340/408] time 0.234 (0.234) data 0.012 (0.013) loss 0.1796 (0.3072) lr 9.8566e-06 eta 0:00:15\n",
            "epoch [50/50] batch [360/408] time 0.230 (0.234) data 0.012 (0.012) loss 0.2349 (0.3090) lr 9.8566e-06 eta 0:00:11\n",
            "epoch [50/50] batch [380/408] time 0.234 (0.234) data 0.013 (0.012) loss 0.2426 (0.3113) lr 9.8566e-06 eta 0:00:06\n",
            "epoch [50/50] batch [400/408] time 0.231 (0.234) data 0.011 (0.012) loss 0.2141 (0.3078) lr 9.8566e-06 eta 0:00:01\n",
            "Using GPA model for final inference...\n",
            "Checkpoint saved to output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1/VLPromptLearner/model.pth.tar-50\n",
            "Finish training\n",
            "Deploy the last-epoch model\n",
            "Evaluate on the *test* set\n",
            "100% 22/22 [00:21<00:00,  1.00it/s]\n",
            "=> result\n",
            "* total: 1,053\n",
            "* correct: 1,042\n",
            "* accuracy: 99.0%\n",
            "* error: 1.0%\n",
            "* macro_f1: 98.8%\n",
            "Elapsed: 1:20:37\n",
            "\\nüìä EVALUATING ON NEW CLASSES...\n",
            "2025-12-09 06:05:31.632263: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765260331.652049   21571 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765260331.657874   21571 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765260331.673233   21571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260331.673259   21571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260331.673262   21571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765260331.673266   21571 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-09 06:05:31.677913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Setting fixed seed: 1\n",
            "***************\n",
            "** Arguments **\n",
            "***************\n",
            "backbone: \n",
            "config_file: configs/trainers/PromptSRC/vit_l14_flowers.yaml\n",
            "dataset_config_file: configs/datasets/oxford_flowers.yaml\n",
            "eval_only: True\n",
            "head: \n",
            "load_epoch: 50\n",
            "model_dir: output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1\n",
            "no_train: False\n",
            "opts: ['DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new', 'DATALOADER.NUM_WORKERS', '0']\n",
            "output_dir: output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1/test_new\n",
            "resume: \n",
            "root: /content/PromptSRC/data\n",
            "seed: 1\n",
            "source_domains: None\n",
            "target_domains: None\n",
            "trainer: PromptSRC\n",
            "transforms: None\n",
            "************\n",
            "** Config **\n",
            "************\n",
            "DATALOADER:\n",
            "  K_TRANSFORMS: 1\n",
            "  NUM_WORKERS: 0\n",
            "  RETURN_IMG0: False\n",
            "  TEST:\n",
            "    BATCH_SIZE: 50\n",
            "    SAMPLER: SequentialSampler\n",
            "  TRAIN_U:\n",
            "    BATCH_SIZE: 32\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAME_AS_X: True\n",
            "    SAMPLER: RandomSampler\n",
            "  TRAIN_X:\n",
            "    BATCH_SIZE: 2\n",
            "    N_DOMAIN: 0\n",
            "    N_INS: 16\n",
            "    SAMPLER: RandomSampler\n",
            "DATASET:\n",
            "  ALL_AS_UNLABELED: False\n",
            "  CIFAR_C_LEVEL: 1\n",
            "  CIFAR_C_TYPE: \n",
            "  NAME: OxfordFlowers\n",
            "  NUM_LABELED: -1\n",
            "  NUM_SHOTS: 16\n",
            "  ROOT: /content/PromptSRC/data\n",
            "  SOURCE_DOMAINS: ()\n",
            "  STL10_FOLD: -1\n",
            "  SUBSAMPLE_CLASSES: new\n",
            "  TARGET_DOMAINS: ()\n",
            "  VAL_PERCENT: 0.1\n",
            "INPUT:\n",
            "  COLORJITTER_B: 0.4\n",
            "  COLORJITTER_C: 0.4\n",
            "  COLORJITTER_H: 0.1\n",
            "  COLORJITTER_S: 0.4\n",
            "  CROP_PADDING: 4\n",
            "  CUTOUT_LEN: 16\n",
            "  CUTOUT_N: 1\n",
            "  GB_K: 21\n",
            "  GB_P: 0.5\n",
            "  GN_MEAN: 0.0\n",
            "  GN_STD: 0.15\n",
            "  INTERPOLATION: bicubic\n",
            "  NO_TRANSFORM: False\n",
            "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
            "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
            "  RANDAUGMENT_M: 10\n",
            "  RANDAUGMENT_N: 2\n",
            "  RGS_P: 0.2\n",
            "  RRCROP_SCALE: (0.08, 1.0)\n",
            "  SIZE: (224, 224)\n",
            "  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')\n",
            "MODEL:\n",
            "  BACKBONE:\n",
            "    NAME: ViT-L/14\n",
            "    PRETRAINED: True\n",
            "  HEAD:\n",
            "    ACTIVATION: relu\n",
            "    BN: True\n",
            "    DROPOUT: 0.0\n",
            "    HIDDEN_LAYERS: ()\n",
            "    NAME: \n",
            "  INIT_WEIGHTS: \n",
            "OPTIM:\n",
            "  ADAM_BETA1: 0.9\n",
            "  ADAM_BETA2: 0.999\n",
            "  BASE_LR_MULT: 0.1\n",
            "  GAMMA: 0.1\n",
            "  LR: 0.0025\n",
            "  LR_SCHEDULER: cosine\n",
            "  MAX_EPOCH: 50\n",
            "  MOMENTUM: 0.9\n",
            "  NAME: sgd\n",
            "  NEW_LAYERS: ()\n",
            "  RMSPROP_ALPHA: 0.99\n",
            "  SGD_DAMPNING: 0\n",
            "  SGD_NESTEROV: False\n",
            "  STAGED_LR: False\n",
            "  STEPSIZE: (-1,)\n",
            "  WARMUP_CONS_LR: 1e-05\n",
            "  WARMUP_EPOCH: 1\n",
            "  WARMUP_MIN_LR: 1e-05\n",
            "  WARMUP_RECOUNT: True\n",
            "  WARMUP_TYPE: constant\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "OUTPUT_DIR: output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1/test_new\n",
            "RESUME: \n",
            "SEED: 1\n",
            "TEST:\n",
            "  COMPUTE_CMAT: False\n",
            "  EVALUATOR: Classification\n",
            "  FINAL_MODEL: last_step\n",
            "  NO_TEST: False\n",
            "  PER_CLASS_RESULT: False\n",
            "  SPLIT: test\n",
            "TRAIN:\n",
            "  CHECKPOINT_FREQ: 0\n",
            "  COUNT_ITER: train_x\n",
            "  PRINT_FREQ: 20\n",
            "TRAINER:\n",
            "  CDAC:\n",
            "    CLASS_LR_MULTI: 10\n",
            "    P_THRESH: 0.95\n",
            "    RAMPUP_COEF: 30\n",
            "    RAMPUP_ITRS: 1000\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    TOPK_MATCH: 5\n",
            "  CROSSGRAD:\n",
            "    ALPHA_D: 0.5\n",
            "    ALPHA_F: 0.5\n",
            "    EPS_D: 1.0\n",
            "    EPS_F: 1.0\n",
            "  DAEL:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DAELDG:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 0.5\n",
            "  DDAIG:\n",
            "    ALPHA: 0.5\n",
            "    CLAMP: False\n",
            "    CLAMP_MAX: 1.0\n",
            "    CLAMP_MIN: -1.0\n",
            "    G_ARCH: \n",
            "    LMDA: 0.3\n",
            "    WARMUP: 0\n",
            "  DOMAINMIX:\n",
            "    ALPHA: 1.0\n",
            "    BETA: 1.0\n",
            "    TYPE: crossdomain\n",
            "  ENTMIN:\n",
            "    LMDA: 0.001\n",
            "  FIXMATCH:\n",
            "    CONF_THRE: 0.95\n",
            "    STRONG_TRANSFORMS: ()\n",
            "    WEIGHT_U: 1.0\n",
            "  M3SDA:\n",
            "    LMDA: 0.5\n",
            "    N_STEP_F: 4\n",
            "  MCD:\n",
            "    N_STEP_F: 4\n",
            "  MEANTEACHER:\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 5\n",
            "    WEIGHT_U: 1.0\n",
            "  MIXMATCH:\n",
            "    MIXUP_BETA: 0.75\n",
            "    RAMPUP: 20000\n",
            "    TEMP: 2.0\n",
            "    WEIGHT_U: 100.0\n",
            "  MME:\n",
            "    LMDA: 0.1\n",
            "  NAME: PromptSRC\n",
            "  PROMPTSRC:\n",
            "    CTX_INIT: a photo of a\n",
            "    GPA_MEAN: 45.0\n",
            "    GPA_STD: 5.0\n",
            "    IMAGE_LOSS_WEIGHT: 10.0\n",
            "    N_CTX: 4\n",
            "    N_CTX_TEXT: 4\n",
            "    N_CTX_VISION: 4\n",
            "    PREC: fp16\n",
            "    PROMPT_DEPTH_TEXT: 9\n",
            "    PROMPT_DEPTH_VISION: 9\n",
            "    TEXT_LOSS_WEIGHT: 25.0\n",
            "  SE:\n",
            "    CONF_THRE: 0.95\n",
            "    EMA_ALPHA: 0.999\n",
            "    RAMPUP: 300\n",
            "USE_CUDA: True\n",
            "VERBOSE: True\n",
            "VERSION: 1\n",
            "Collecting env info ...\n",
            "** System info **\n",
            "PyTorch version: 2.9.0+cu126\n",
            "Is debug build: False\n",
            "CUDA used to build PyTorch: 12.6\n",
            "ROCM used to build PyTorch: N/A\n",
            "\n",
            "OS: Ubuntu 22.04.4 LTS (x86_64)\n",
            "GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0\n",
            "Clang version: Could not collect\n",
            "CMake version: version 3.31.10\n",
            "Libc version: glibc-2.35\n",
            "\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0] (64-bit runtime)\n",
            "Python platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Is CUDA available: True\n",
            "CUDA runtime version: 12.5.82\n",
            "CUDA_MODULE_LOADING set to: \n",
            "GPU models and configuration: GPU 0: Tesla T4\n",
            "Nvidia driver version: 550.54.15\n",
            "cuDNN version: Probably one of the following:\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.2.1\n",
            "/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.2.1\n",
            "Is XPU available: False\n",
            "HIP runtime version: N/A\n",
            "MIOpen runtime version: N/A\n",
            "Is XNNPACK available: True\n",
            "\n",
            "CPU:\n",
            "Architecture:                            x86_64\n",
            "CPU op-mode(s):                          32-bit, 64-bit\n",
            "Address sizes:                           46 bits physical, 48 bits virtual\n",
            "Byte Order:                              Little Endian\n",
            "CPU(s):                                  2\n",
            "On-line CPU(s) list:                     0,1\n",
            "Vendor ID:                               GenuineIntel\n",
            "Model name:                              Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "CPU family:                              6\n",
            "Model:                                   85\n",
            "Thread(s) per core:                      2\n",
            "Core(s) per socket:                      1\n",
            "Socket(s):                               1\n",
            "Stepping:                                3\n",
            "BogoMIPS:                                4000.24\n",
            "Flags:                                   fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Hypervisor vendor:                       KVM\n",
            "Virtualization type:                     full\n",
            "L1d cache:                               32 KiB (1 instance)\n",
            "L1i cache:                               32 KiB (1 instance)\n",
            "L2 cache:                                1 MiB (1 instance)\n",
            "L3 cache:                                38.5 MiB (1 instance)\n",
            "NUMA node(s):                            1\n",
            "NUMA node0 CPU(s):                       0,1\n",
            "Vulnerability Gather data sampling:      Not affected\n",
            "Vulnerability Indirect target selection: Vulnerable\n",
            "Vulnerability Itlb multihit:             Not affected\n",
            "Vulnerability L1tf:                      Mitigation; PTE Inversion\n",
            "Vulnerability Mds:                       Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:                  Vulnerable\n",
            "Vulnerability Mmio stale data:           Vulnerable\n",
            "Vulnerability Reg file data sampling:    Not affected\n",
            "Vulnerability Retbleed:                  Vulnerable\n",
            "Vulnerability Spec rstack overflow:      Not affected\n",
            "Vulnerability Spec store bypass:         Vulnerable\n",
            "Vulnerability Spectre v1:                Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:                Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BHI: Vulnerable\n",
            "Vulnerability Srbds:                     Not affected\n",
            "Vulnerability Tsa:                       Not affected\n",
            "Vulnerability Tsx async abort:           Vulnerable\n",
            "\n",
            "Versions of relevant libraries:\n",
            "[pip3] flake8==3.7.9\n",
            "[pip3] intel-cmplr-lib-ur==2025.3.1\n",
            "[pip3] intel-openmp==2025.3.1\n",
            "[pip3] mkl==2025.3.0\n",
            "[pip3] numpy==1.26.4\n",
            "[pip3] nvidia-cublas-cu12==12.6.4.1\n",
            "[pip3] nvidia-cuda-cupti-cu12==12.6.80\n",
            "[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n",
            "[pip3] nvidia-cuda-runtime-cu12==12.6.77\n",
            "[pip3] nvidia-cudnn-cu12==9.10.2.21\n",
            "[pip3] nvidia-cufft-cu12==11.3.0.4\n",
            "[pip3] nvidia-curand-cu12==10.3.7.77\n",
            "[pip3] nvidia-cusolver-cu12==11.7.1.2\n",
            "[pip3] nvidia-cusparse-cu12==12.5.4.2\n",
            "[pip3] nvidia-cusparselt-cu12==0.7.1\n",
            "[pip3] nvidia-nccl-cu12==2.27.5\n",
            "[pip3] nvidia-nvjitlink-cu12==12.6.85\n",
            "[pip3] nvidia-nvtx-cu12==12.6.77\n",
            "[pip3] nvtx==0.2.14\n",
            "[pip3] onemkl-license==2025.3.0\n",
            "[pip3] optree==0.18.0\n",
            "[pip3] tbb==2022.3.0\n",
            "[pip3] tcmlib==1.4.1\n",
            "[pip3] torch==2.9.0+cu126\n",
            "[pip3] torchao==0.10.0\n",
            "[pip3] torchaudio==2.9.0+cu126\n",
            "[pip3] torchdata==0.11.0\n",
            "[pip3] torchsummary==1.5.1\n",
            "[pip3] torchtune==0.6.1\n",
            "[pip3] torchvision==0.24.0+cu126\n",
            "[pip3] triton==3.5.0\n",
            "[pip3] umf==1.0.2\n",
            "[conda] Could not collect\n",
            "        Pillow (11.3.0)\n",
            "Loading trainer: PromptSRC\n",
            "Loading dataset: OxfordFlowers\n",
            "Reading split from /content/PromptSRC/data/oxford_flowers/split_zhou_OxfordFlowers.json\n",
            "Loading preprocessed few-shot data from /content/PromptSRC/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl\n",
            "SUBSAMPLE NEW CLASSES!\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "---------  -------------\n",
            "Dataset    OxfordFlowers\n",
            "# classes  51\n",
            "# train_x  816\n",
            "# val      204\n",
            "# test     1,410\n",
            "---------  -------------\n",
            "Loading CLIP (backbone: ViT-L/14)\n",
            "Weights not found for some missing keys:  ['visual.VPT', 'visual.transformer.resblocks.1.VPT_shallow', 'visual.transformer.resblocks.2.VPT_shallow', 'visual.transformer.resblocks.3.VPT_shallow', 'visual.transformer.resblocks.4.VPT_shallow', 'visual.transformer.resblocks.5.VPT_shallow', 'visual.transformer.resblocks.6.VPT_shallow', 'visual.transformer.resblocks.7.VPT_shallow', 'visual.transformer.resblocks.8.VPT_shallow', 'transformer.resblocks.1.VPT_shallow', 'transformer.resblocks.2.VPT_shallow', 'transformer.resblocks.3.VPT_shallow', 'transformer.resblocks.4.VPT_shallow', 'transformer.resblocks.5.VPT_shallow', 'transformer.resblocks.6.VPT_shallow', 'transformer.resblocks.7.VPT_shallow', 'transformer.resblocks.8.VPT_shallow']\n",
            "Building custom CLIP\n",
            "Independent V-L design\n",
            "Initial text context: \"a photo of a\"\n",
            "Number of context words (tokens) for Language prompting: 4\n",
            "Number of context words (tokens) for Vision prompting: 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Parameters to be updated: {'image_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.1.VPT_shallow', 'text_encoder.transformer.resblocks.5.VPT_shallow', 'image_encoder.transformer.resblocks.3.VPT_shallow', 'prompt_learner.ctx', 'image_encoder.transformer.resblocks.5.VPT_shallow', 'text_encoder.transformer.resblocks.7.VPT_shallow', 'text_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.4.VPT_shallow', 'text_encoder.transformer.resblocks.3.VPT_shallow', 'image_encoder.transformer.resblocks.2.VPT_shallow', 'image_encoder.VPT', 'text_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.8.VPT_shallow', 'image_encoder.transformer.resblocks.6.VPT_shallow', 'text_encoder.transformer.resblocks.4.VPT_shallow', 'image_encoder.transformer.resblocks.1.VPT_shallow'}\n",
            "Parameters count: 18\n",
            "Loading evaluator: Classification\n",
            "Loading weights to VLPromptLearner from \"output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1/VLPromptLearner/model.pth.tar-50\" (epoch = 50)\n",
            "Evaluate on the *test* set\n",
            "100% 29/29 [00:31<00:00,  1.10s/it]\n",
            "=> result\n",
            "* total: 1,410\n",
            "* correct: 1,166\n",
            "* accuracy: 82.7%\n",
            "* error: 17.3%\n",
            "* macro_f1: 78.5%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "%cd /content\n",
        "!rm -rf PromptSRC Dassl.pytorch\n",
        "!pip uninstall -y shap > /dev/null 2>&1\n",
        "!pip install \"numpy<2.0\" gdown > /dev/null 2>&1\n",
        "\n",
        "# Clone Repos\n",
        "!git clone https://github.com/muzairkhattak/PromptSRC.git\n",
        "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
        "\n",
        "%cd /content/Dassl.pytorch\n",
        "torchtools_path = \"dassl/utils/torchtools.py\"\n",
        "with open(torchtools_path, \"r\") as f:\n",
        "    code = f.read()\n",
        "if \"weights_only\" not in code:\n",
        "    code = code.replace(\"checkpoint = torch.load(fpath, map_location=map_location)\",\n",
        "                        \"checkpoint = torch.load(fpath, map_location=map_location, weights_only=False)\")\n",
        "with open(torchtools_path, \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "# Install Dassl\n",
        "!pip install -r requirements.txt > /dev/null 2>&1\n",
        "!python setup.py develop > /dev/null 2>&1\n",
        "\n",
        "# Install PromptSRC\n",
        "%cd /content/PromptSRC\n",
        "!pip install -r requirements.txt > /dev/null 2>&1\n",
        "\n",
        "psrc_path = \"/content/PromptSRC/trainers/promptsrc.py\"\n",
        "vit_l14_url = \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"\n",
        "\n",
        "with open(psrc_path, \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# We replace the lookup line with a safe block\n",
        "target_code = \"    url = clip._MODELS[backbone_name]\"\n",
        "patch_code = f\"\"\"    try:\n",
        "        url = clip._MODELS[backbone_name]\n",
        "    except KeyError:\n",
        "        if backbone_name == 'ViT-L/14':\n",
        "            url = \"{vit_l14_url}\"\n",
        "        else:\n",
        "            raise\"\"\"\n",
        "\n",
        "if target_code in content:\n",
        "    content = content.replace(target_code, patch_code)\n",
        "    with open(psrc_path, \"w\") as f:\n",
        "        f.write(content)\n",
        "    print(\"Library patched successfully.\")\n",
        "else:\n",
        "    print(\"Patch already applied or file changed.\")\n",
        "\n",
        "\n",
        "DATA_ROOT = \"/content/PromptSRC/data\"\n",
        "FLOWERS_DIR = os.path.join(DATA_ROOT, \"oxford_flowers\")\n",
        "os.makedirs(FLOWERS_DIR, exist_ok=True)\n",
        "USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n",
        "\n",
        "# Download Flowers\n",
        "!curl -L -A \"{USER_AGENT}\" -o {DATA_ROOT}/102flowers.tgz https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!curl -L -A \"{USER_AGENT}\" -o {FLOWERS_DIR}/imagelabels.mat https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
        "!curl -L -A \"{USER_AGENT}\" -o {FLOWERS_DIR}/setid.mat https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat\n",
        "!tar -xf {DATA_ROOT}/102flowers.tgz -C {FLOWERS_DIR}\n",
        "\n",
        "# Download & Fix JSON\n",
        "json_path = os.path.join(FLOWERS_DIR, \"cat_to_name.json\")\n",
        "if os.path.exists(json_path):\n",
        "    os.remove(json_path)\n",
        "!wget -q -O {json_path} https://raw.githubusercontent.com/udacity/aipnd-project/master/cat_to_name.json\n",
        "\n",
        "\n",
        "config_dir = \"configs/trainers/PromptSRC\"\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "config_file = os.path.join(config_dir, \"vit_l14_flowers.yaml\")\n",
        "\n",
        "config_content = \"\"\"\n",
        "DATALOADER:\n",
        "  TRAIN_X:\n",
        "    BATCH_SIZE: 2\n",
        "  TEST:\n",
        "    BATCH_SIZE: 50\n",
        "  NUM_WORKERS: 0\n",
        "\n",
        "INPUT:\n",
        "  SIZE: (224, 224)\n",
        "  INTERPOLATION: \"bicubic\"\n",
        "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
        "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
        "  TRANSFORMS: [\"random_resized_crop\", \"random_flip\", \"normalize\"]\n",
        "\n",
        "OPTIM:\n",
        "  NAME: \"sgd\"\n",
        "  LR: 0.0025\n",
        "  MAX_EPOCH: 50\n",
        "  LR_SCHEDULER: \"cosine\"\n",
        "  WARMUP_EPOCH: 1\n",
        "  WARMUP_TYPE: \"constant\"\n",
        "  WARMUP_CONS_LR: 1e-5\n",
        "\n",
        "TRAIN:\n",
        "  PRINT_FREQ: 20\n",
        "\n",
        "MODEL:\n",
        "  BACKBONE:\n",
        "    NAME: \"ViT-L/14\"\n",
        "\n",
        "TRAINER:\n",
        "  PROMPTSRC:\n",
        "    N_CTX: 4\n",
        "    N_CTX_VISION: 4\n",
        "    N_CTX_TEXT: 4\n",
        "    CTX_INIT: \"a photo of a\"\n",
        "    PREC: \"fp16\"\n",
        "    PROMPT_DEPTH_TEXT: 9\n",
        "    PROMPT_DEPTH_VISION: 9\n",
        "    TEXT_LOSS_WEIGHT: 25.0\n",
        "    IMAGE_LOSS_WEIGHT: 10.0\n",
        "    GPA_MEAN: 45.0\n",
        "    GPA_STD: 5.0\n",
        "\"\"\"\n",
        "with open(config_file, \"w\") as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "!sed -i 's/super().__init__(optimizer, last_epoch, verbose)/super().__init__(optimizer, last_epoch)/' /content/Dassl.pytorch/dassl/optim/lr_scheduler.py\n",
        "\n",
        "\n",
        "new_train_code = \"\"\"\n",
        "import argparse\n",
        "import torch\n",
        "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
        "from dassl.config import get_cfg_default\n",
        "from dassl.engine import build_trainer\n",
        "from yacs.config import CfgNode as CN\n",
        "\n",
        "# --- IMPORTS ---\n",
        "from trainers import promptsrc\n",
        "from datasets import oxford_flowers\n",
        "# ---------------\n",
        "\n",
        "def print_args(args, cfg):\n",
        "    print(\"***************\")\n",
        "    print(\"** Arguments **\")\n",
        "    print(\"***************\")\n",
        "    optkeys = list(args.__dict__.keys())\n",
        "    optkeys.sort()\n",
        "    for key in optkeys:\n",
        "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
        "    print(\"************\")\n",
        "    print(\"** Config **\")\n",
        "    print(\"************\")\n",
        "    print(cfg)\n",
        "\n",
        "def reset_cfg(cfg, args):\n",
        "    if args.root:\n",
        "        cfg.DATASET.ROOT = args.root\n",
        "    if args.output_dir:\n",
        "        cfg.OUTPUT_DIR = args.output_dir\n",
        "    if args.resume:\n",
        "        cfg.RESUME = args.resume\n",
        "    if args.seed:\n",
        "        cfg.SEED = args.seed\n",
        "    if args.source_domains:\n",
        "        cfg.DATASET.SOURCE_DOMAINS = args.source_domains\n",
        "    if args.target_domains:\n",
        "        cfg.DATASET.TARGET_DOMAINS = args.target_domains\n",
        "    if args.transforms:\n",
        "        cfg.INPUT.TRANSFORMS = args.transforms\n",
        "    if args.trainer:\n",
        "        cfg.TRAINER.NAME = args.trainer\n",
        "    if args.backbone:\n",
        "        cfg.MODEL.BACKBONE.NAME = args.backbone\n",
        "    if args.head:\n",
        "        cfg.MODEL.HEAD.NAME = args.head\n",
        "\n",
        "def extend_cfg(cfg):\n",
        "    # Register PromptSRC Keys\n",
        "    if not hasattr(cfg.TRAINER, \"PROMPTSRC\"):\n",
        "            cfg.TRAINER.PROMPTSRC = CN()\n",
        "            # Dimensions updated for V-L prompting\n",
        "            cfg.TRAINER.PROMPTSRC.N_CTX = 4\n",
        "            cfg.TRAINER.PROMPTSRC.N_CTX_VISION = 4\n",
        "            cfg.TRAINER.PROMPTSRC.N_CTX_TEXT = 4\n",
        "\n",
        "            cfg.TRAINER.PROMPTSRC.CTX_INIT = \"\"\n",
        "            cfg.TRAINER.PROMPTSRC.PREC = \"fp16\"\n",
        "            cfg.TRAINER.PROMPTSRC.PROMPT_DEPTH_TEXT = 9\n",
        "            cfg.TRAINER.PROMPTSRC.PROMPT_DEPTH_VISION = 9\n",
        "            cfg.TRAINER.PROMPTSRC.TEXT_LOSS_WEIGHT = 25.0\n",
        "            cfg.TRAINER.PROMPTSRC.IMAGE_LOSS_WEIGHT = 10.0\n",
        "            cfg.TRAINER.PROMPTSRC.GPA_MEAN = 0.1\n",
        "            cfg.TRAINER.PROMPTSRC.GPA_STD = 0.1\n",
        "\n",
        "    # Register Subsample Key\n",
        "    if not hasattr(cfg.DATASET, \"SUBSAMPLE_CLASSES\"):\n",
        "        cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"\n",
        "\n",
        "def setup_cfg(args):\n",
        "    cfg = get_cfg_default()\n",
        "    extend_cfg(cfg)\n",
        "    reset_cfg(cfg, args)\n",
        "    if args.dataset_config_file:\n",
        "        cfg.merge_from_file(args.dataset_config_file)\n",
        "    if args.config_file:\n",
        "        cfg.merge_from_file(args.config_file)\n",
        "    cfg.merge_from_list(args.opts)\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "def main(args):\n",
        "    cfg = setup_cfg(args)\n",
        "    if cfg.SEED >= 0:\n",
        "        print(\"Setting fixed seed: {}\".format(cfg.SEED))\n",
        "        set_random_seed(cfg.SEED)\n",
        "    setup_logger(cfg.OUTPUT_DIR)\n",
        "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    print_args(args, cfg)\n",
        "    print(\"Collecting env info ...\")\n",
        "    print(\"** System info **\\\\n{}\".format(collect_env_info()))\n",
        "    trainer = build_trainer(cfg)\n",
        "    if args.eval_only:\n",
        "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
        "        trainer.test()\n",
        "        return\n",
        "    if not args.no_train:\n",
        "        trainer.train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--root\", type=str, default=\"\", help=\"path to dataset\")\n",
        "    parser.add_argument(\"--output-dir\", type=str, default=\"\", help=\"output directory\")\n",
        "    parser.add_argument(\"--resume\", type=str, default=\"\", help=\"checkpoint directory\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=-1, help=\"only positive value enables a fixed seed\")\n",
        "    parser.add_argument(\"--source-domains\", type=str, nargs=\"+\", help=\"source domains for DA/DG\")\n",
        "    parser.add_argument(\"--target-domains\", type=str, nargs=\"+\", help=\"target domains for DA/DG\")\n",
        "    parser.add_argument(\"--transforms\", type=str, nargs=\"+\", help=\"data augmentation transforms\")\n",
        "    parser.add_argument(\"--config-file\", type=str, default=\"\", help=\"path to config file\")\n",
        "    parser.add_argument(\"--dataset-config-file\", type=str, default=\"\", help=\"path to config file for dataset\")\n",
        "    parser.add_argument(\"--trainer\", type=str, default=\"\", help=\"name of trainer\")\n",
        "    parser.add_argument(\"--backbone\", type=str, default=\"\", help=\"name of CNN backbone\")\n",
        "    parser.add_argument(\"--head\", type=str, default=\"\", help=\"name of head\")\n",
        "    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
        "    parser.add_argument(\"--model-dir\", type=str, default=\"\", help=\"load model from this directory\")\n",
        "    parser.add_argument(\"--load-epoch\", type=int, help=\"load model weights at this epoch\")\n",
        "    parser.add_argument(\"--no-train\", action=\"store_true\", help=\"do not train\")\n",
        "    parser.add_argument(\"opts\", default=None, nargs=argparse.REMAINDER, help=\"modify config options\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"train.py\", \"w\") as f:\n",
        "    f.write(new_train_code)\n",
        "\n",
        "\n",
        "script_path = \"scripts/run_vit_l14.sh\"\n",
        "script_content = f\"\"\"#!/bin/bash\n",
        "DATASET=oxford_flowers\n",
        "SEED=1\n",
        "CFG=vit_l14_flowers\n",
        "SHOTS=16\n",
        "OUTPUT_DIR=output/base2new/vit_l14_result/oxford_flowers/shots_16/PromptSRC/seed1\n",
        "\n",
        "python train.py \\\\\n",
        "--root /content/PromptSRC/data \\\\\n",
        "--seed ${{SEED}} \\\\\n",
        "--trainer PromptSRC \\\\\n",
        "--dataset-config-file configs/datasets/${{DATASET}}.yaml \\\\\n",
        "--config-file configs/trainers/PromptSRC/${{CFG}}.yaml \\\\\n",
        "--output-dir ${{OUTPUT_DIR}} \\\\\n",
        "DATASET.NUM_SHOTS ${{SHOTS}} \\\\\n",
        "DATASET.SUBSAMPLE_CLASSES base \\\\\n",
        "DATALOADER.NUM_WORKERS 0\n",
        "\n",
        "python train.py \\\\\n",
        "--root /content/PromptSRC/data \\\\\n",
        "--seed ${{SEED}} \\\\\n",
        "--trainer PromptSRC \\\\\n",
        "--dataset-config-file configs/datasets/${{DATASET}}.yaml \\\\\n",
        "--config-file configs/trainers/PromptSRC/${{CFG}}.yaml \\\\\n",
        "--output-dir ${{OUTPUT_DIR}}/test_new \\\\\n",
        "--model-dir ${{OUTPUT_DIR}} \\\\\n",
        "--load-epoch 50 \\\\\n",
        "--eval-only \\\\\n",
        "DATASET.NUM_SHOTS ${{SHOTS}} \\\\\n",
        "DATASET.SUBSAMPLE_CLASSES new \\\\\n",
        "DATALOADER.NUM_WORKERS 0\n",
        "\"\"\"\n",
        "with open(script_path, \"w\") as f:\n",
        "    f.write(script_content)\n",
        "\n",
        "!bash scripts/run_vit_l14.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-Y2moyOZ6NM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aExXuO4ZxMX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}